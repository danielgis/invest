"""Module for Regression Testing the InVEST Coastal Vulnerability module."""
import unittest
import tempfile
import shutil
import os
import pickle
import json

from osgeo import gdal, osr, ogr
import numpy.testing
import pandas.testing
from pygeoprocessing.testing import sampledata
from shapely.geometry import Point, Polygon, MultiPolygon
import taskgraph

from natcap.invest import coastal_vulnerability

REGRESSION_DATA = os.path.join(
    os.path.dirname(__file__), '..', 'data', 'invest-test-data',
    'coastal_vulnerability')
INPUT_DATA = os.path.join(
    os.path.dirname(__file__), '..', 'data', 'invest-test-data',
    'coastal_vulnerability', 'input')


class CoastalVulnerabilityTests(unittest.TestCase):
    """Tests for the Coastal Vulnerability Model."""

    def setUp(self):
        """Overriding setUp function to create temp workspace directory."""
        # this lets us delete the workspace after its done no matter the
        # the rest result
        self.workspace_dir = tempfile.mkdtemp()

    def tearDown(self):
        """Overriding tearDown function to remove temporary directory."""
        shutil.rmtree(self.workspace_dir)

    @staticmethod
    def generate_base_args(workspace_dir):
        """Generate an args dict with default required args."""
        args = {'workspace_dir': workspace_dir,
                'n_workers': -1,
                'wwiii_vector_path': os.path.join(
                    INPUT_DATA, 'WaveWatchIII_subset.shp'),
                'landmass_vector_path': os.path.join(
                    INPUT_DATA, 'land_polygon_simple_utm.shp'),
                'aoi_vector_path': os.path.join(
                    INPUT_DATA, 'AOI_BarkClay.shp'),
                'model_resolution': 25000,
                'max_fetch_distance': 12000,
                'dem_path': os.path.join(
                    INPUT_DATA, 'dem_wgs84.tif'),
                'dem_averaging_radius': 33000.0,
                'habitat_table_path': os.path.join(
                    INPUT_DATA, "natural_habitats_wcvi.csv"),
                'shelf_contour_vector_path': os.path.join(
                    INPUT_DATA, 'continental_shelf_contour.gpkg')
                }
        return args

    def test_wind_and_wave_exposure(self):
        """CV: regression test for wind and wave exposure values.

        The wave calculation depends on an intermediate product from
        the wind calculation, so I'm testing them both in this scope."""

        workspace_dir = self.workspace_dir
        # these points have the WWIII values interpolated onto them.
        base_shore_point_vector_path = os.path.join(
            INPUT_DATA, "wwiii_shore_points_5000m.gpkg")
        max_fetch_distance = 12000
        target_fetch_point_vector_path = os.path.join(
            workspace_dir, 'fetch_points.gpkg')
        target_fetch_rays_vector_path = os.path.join(
            workspace_dir, 'fetch_rays.gpkg')
        target_wind_exposure_pickle_path = os.path.join(
            workspace_dir, 'wind.pickle')

        landmass_vector_path = os.path.join(
            INPUT_DATA, 'land_polygon_simple_utm.shp')
        landmass_polygon_pickle_path = os.path.join(
            self.workspace_dir, 'polygon.pickle')
        landmass_lines_pickle_path = os.path.join(
            self.workspace_dir, 'lines.pickle')
        landmass_lines_rtree_path = os.path.join(
            self.workspace_dir, 'lines_rtree.dat')

        coastal_vulnerability.prepare_landmass_line_index(
            landmass_vector_path, landmass_polygon_pickle_path,
            landmass_lines_pickle_path, landmass_lines_rtree_path)

        coastal_vulnerability.calculate_wind_exposure(
            base_shore_point_vector_path,
            landmass_polygon_pickle_path,
            landmass_lines_rtree_path,
            landmass_lines_pickle_path,
            target_fetch_rays_vector_path,
            max_fetch_distance,
            target_fetch_point_vector_path,
            target_wind_exposure_pickle_path)

        expected_raw_values_path = os.path.join(
            REGRESSION_DATA, 'expected_wind.json')
        assert_pickled_arrays_almost_equal(
            target_wind_exposure_pickle_path, expected_raw_values_path)

        target_wave_exposure_pickle_path = os.path.join(
            workspace_dir, 'wave.pickle')
        coastal_vulnerability.calculate_wave_exposure(
            target_fetch_point_vector_path, max_fetch_distance,
            os.path.join(self.workspace_dir, 'intermediate_wave.gpkg'),
            target_wave_exposure_pickle_path)

        expected_raw_values_path = os.path.join(
            REGRESSION_DATA, 'expected_wave.json')
        assert_pickled_arrays_almost_equal(
            target_wave_exposure_pickle_path, expected_raw_values_path)

    def test_habitat_rank(self):
        """CV: regression test for habitat ranks."""

        workspace_dir = self.workspace_dir
        base_shore_point_vector_path = os.path.join(
            INPUT_DATA, "wwiii_shore_points_5000m.gpkg")
        habitat_table_path = os.path.join(
            INPUT_DATA, "natural_habitats_wcvi.csv")
        target_habitat_protection_path = os.path.join(
            workspace_dir, 'habitat_protection.csv')
        file_suffix = ''

        task_graph = taskgraph.TaskGraph(
            os.path.join(workspace_dir, 'taskgraph_dir'), -1)

        task_list, pickle_list = coastal_vulnerability._schedule_habitat_tasks(
            base_shore_point_vector_path, habitat_table_path,
            workspace_dir, file_suffix, task_graph)

        coastal_vulnerability.calculate_habitat_rank(
            pickle_list, target_habitat_protection_path)

        expected_habitat_path = os.path.join(
            REGRESSION_DATA, 'expected_habitat_protection.csv')
        actual_values_df = pandas.read_csv(target_habitat_protection_path)
        expected_values_df = pandas.read_csv(expected_habitat_path)
        pandas.testing.assert_frame_equal(actual_values_df, expected_values_df)

    def test_geomorphology_rank(self):
        """CV: regression test for geomorphology values."""

        workspace_dir = self.workspace_dir
        geomorphology_vector_path = os.path.join(
            INPUT_DATA, "geomorphology_few_ranks.shp")
        base_shore_point_vector_path = os.path.join(
            INPUT_DATA, "wwiii_shore_points_5000m.gpkg")
        target_pickle_path = os.path.join(
            workspace_dir, 'geomorphology.pickle')
        model_resolution = 5000

        coastal_vulnerability.calculate_geomorphology_exposure(
            geomorphology_vector_path, 3,
            base_shore_point_vector_path,
            model_resolution, target_pickle_path)

        expected_values_pickle_path = os.path.join(
            REGRESSION_DATA, 'expected_geomorphology.json')
        assert_pickled_arrays_almost_equal(
            target_pickle_path, expected_values_pickle_path)

    def test_creation_of_missing_geomorphology_dataset(self):
        """CV: test vector is created when some points are missing geomorph."""

        workspace_dir = self.workspace_dir
        geomorphology_vector_path = os.path.join(
            INPUT_DATA, "geomorphology_few_ranks.shp")
        srs = osr.SpatialReference()
        srs.ImportFromEPSG(26910)  # UTM Zone 10N
        simple_points_path = os.path.join(self.workspace_dir, 'simple_points.gpkg')
        # create a point vector very far from any geomorphology segments:
        point = [Point(0.0, 0.0)]
        sampledata.create_vector_on_disk(
            point, srs.ExportToWkt(),
            filename=simple_points_path, vector_format='GPKG')
        target_pickle_path = os.path.join(
            workspace_dir, 'geomorphology.pickle')
        model_resolution = 5  # makes for very small search radius

        coastal_vulnerability.calculate_geomorphology_exposure(
            geomorphology_vector_path, 3,
            simple_points_path,
            model_resolution, target_pickle_path)

        expected_file_path = os.path.join(
            os.path.dirname(target_pickle_path),
            "shore_points_missing_geomorphology.gpkg")
        vector = gdal.OpenEx(expected_file_path, gdal.OF_VECTOR)
        layer = vector.GetLayer()
        n_features = layer.GetFeatureCount()
        layer = None
        vector = None
        self.assertTrue(len(point) == n_features)

    def test_surge_exposure_rank(self):
        """CV: regression test for surge exposure values."""

        workspace_dir = self.workspace_dir
        base_shore_point_vector_path = os.path.join(
            INPUT_DATA, "wwiii_shore_points_5000m.gpkg")
        shelf_contour_path = os.path.join(
            INPUT_DATA, 'continental_shelf_contour.gpkg')
        aoi_vector_path = os.path.join(
            INPUT_DATA, 'AOI_BarkClay.shp')
        target_surge_pickle_path = os.path.join(
            workspace_dir, 'surge.pickle')

        coastal_vulnerability.calculate_surge_exposure(
            base_shore_point_vector_path, shelf_contour_path,
            aoi_vector_path, target_surge_pickle_path)

        expected_raw_values_path = os.path.join(
            REGRESSION_DATA, 'expected_surge.json')

        assert_pickled_arrays_almost_equal(
            target_surge_pickle_path, expected_raw_values_path)

    def test_relief_values(self):
        """CV: regression test for aggregated relief values."""

        workspace_dir = self.workspace_dir
        base_shore_point_vector_path = os.path.join(
            INPUT_DATA, 'wwiii_shore_points_5000m.gpkg')
        dem_path = os.path.join(
            INPUT_DATA, 'dem_wgs84.tif')
        target_relief_pickle_path = os.path.join(
            workspace_dir, 'relief.pickle')
        dem_averaging_radius = 20000.0
        model_resolution = 5000.0
        file_suffix = ''

        coastal_vulnerability.calculate_relief_exposure(
            base_shore_point_vector_path, dem_path, dem_averaging_radius,
            model_resolution, workspace_dir, file_suffix,
            target_relief_pickle_path)

        expected_raw_values_path = os.path.join(
            REGRESSION_DATA, 'expected_relief.json')

        # I found minor gdal version differences produced slightly different
        # pixel values from gdal.Warp. So asserting exact values
        # from calculate_relief_exposure might fail depending on gdal version.
        # Rather than pin this test to a specific gdal version, I'm asserting
        # equal rank values after binning the raw values into 1-5 ranks.
        with open(target_relief_pickle_path, 'rb') as file:
            actual_values_dict = pickle.load(file)
        actual_rank_dict = coastal_vulnerability._bin_values_to_percentiles(
            actual_values_dict, invert_values=True)
        with open(expected_raw_values_path, 'r') as file:
            expected_values_dict = json.load(file)
        expected_rank_dict = coastal_vulnerability._bin_values_to_percentiles(
            expected_values_dict, invert_values=True)
        expected_rank_dict = {
            int(x[0]): int(x[1]) for x in expected_rank_dict.items()}
        # the dict items need sorting by FID to match the pre-sorted pickled items
        expected_ranks = [x[1] for x in sorted(expected_rank_dict.items())]

        numpy.testing.assert_array_equal(
            list(actual_rank_dict.values()), expected_ranks)

    def test_population_values(self):
        """CV: regression test for aggregated population density."""

        workspace_dir = self.workspace_dir
        base_shore_point_vector_path = os.path.join(
            INPUT_DATA, 'wwiii_shore_points_5000m.gpkg')
        population_path = os.path.join(
            INPUT_DATA, 'population.tif')
        target_relief_pickle_path = os.path.join(
            workspace_dir, 'population.pickle')
        search_radius = 20000.0
        model_resolution = 5000.0
        file_suffix = ''

        coastal_vulnerability.aggregate_population_density(
            base_shore_point_vector_path, population_path, search_radius,
            model_resolution, workspace_dir, file_suffix,
            target_relief_pickle_path)

        expected_raw_values_path = os.path.join(
            REGRESSION_DATA, 'expected_population.json')

        assert_pickled_arrays_almost_equal(
            target_relief_pickle_path, expected_raw_values_path)

    def test_interpolate_slr(self):
        """CV: regression test for sea-level rise values.

        This tests an edge case where there is only one point in the
        SLR dataset, and it requires a coordinate transformation.
        """

        workspace_dir = self.workspace_dir
        base_shore_point_vector_path = os.path.join(
            INPUT_DATA, 'wwiii_shore_points_5000m.gpkg')

        # Make an SLR point vector
        slr_fieldname = 'Trend'
        slr_point_vector_path = os.path.join(workspace_dir, 'simple_points.shp')
        out_driver = ogr.GetDriverByName('ESRI Shapefile')
        srs = osr.SpatialReference()
        srs.ImportFromEPSG(4326)
        shapely_feature = Point(-125.65, 49.0)
        out_vector = out_driver.CreateDataSource(slr_point_vector_path)
        layer_name = os.path.basename(os.path.splitext(slr_point_vector_path)[0])
        out_layer = out_vector.CreateLayer(layer_name, srs=srs)
        field_defn = ogr.FieldDefn(slr_fieldname, ogr.OFTReal)
        out_layer.CreateField(field_defn)
        layer_defn = out_layer.GetLayerDefn()
        new_feature = ogr.Feature(layer_defn)
        new_geometry = ogr.CreateGeometryFromWkb(shapely_feature.wkb)
        new_feature.SetGeometry(new_geometry)
        new_feature.SetField(slr_fieldname, 1.3)
        out_layer.CreateFeature(new_feature)
        out_layer = None
        out_vector = None

        target_pickle_path = os.path.join(
            workspace_dir, 'slr.pickle')
        coastal_vulnerability.interpolate_sealevelrise_points(
            base_shore_point_vector_path, slr_point_vector_path,
            slr_fieldname, target_pickle_path)

        expected_raw_values_path = os.path.join(
            REGRESSION_DATA, 'expected_slr.json')

        assert_pickled_arrays_almost_equal(
            target_pickle_path, expected_raw_values_path)

    def test_interpolate_slr_beyond_maxdistance(self):
        """CV: test sea-level rise returns nan beyond max search distance."""

        workspace_dir = self.workspace_dir

        # Make a shore point
        srs = osr.SpatialReference()
        srs.ImportFromEPSG(26910)  # UTM Zone 10N
        shore_point_path = os.path.join(workspace_dir, 'shore_point.shp')
        sampledata.create_vector_on_disk(
            [Point(0., 0.)], srs.ExportToWkt(), filename=shore_point_path)

        slr_point_vector_path = os.path.join(workspace_dir, 'slr_point.shp')
        slr_fieldname = 'Trend'
        shapely_point = Point(1e6, 1e6)  # very far from the shore point
        make_slr_vector(
            slr_point_vector_path, slr_fieldname, shapely_point, srs)

        target_pickle_path = os.path.join(
            workspace_dir, 'slr.pickle')
        coastal_vulnerability.interpolate_sealevelrise_points(
            shore_point_path, slr_point_vector_path,
            slr_fieldname, target_pickle_path)

        with open(target_pickle_path, 'rb') as file:
            actual_values = pickle.load(file)
        expected_values = numpy.array([numpy.nan])

        numpy.testing.assert_almost_equal(
            list(actual_values.values()), expected_values, decimal=4)

    def test_slr_missing_field(self):
        """CV: test KeyError raised if slr field is not present in vector."""
        workspace_dir = self.workspace_dir
        # Make a shore point
        srs = osr.SpatialReference()
        srs.ImportFromEPSG(26910)  # UTM Zone 10N
        shore_point_path = os.path.join(workspace_dir, 'shore_point.shp')
        sampledata.create_vector_on_disk(
            [Point(0., 0.)], srs.ExportToWkt(), filename=shore_point_path)

        slr_point_vector_path = os.path.join(workspace_dir, 'slr_point.shp')
        slr_fieldname = 'Trend'
        shapely_point = Point(1., 1.)
        make_slr_vector(
            slr_point_vector_path, slr_fieldname, shapely_point, srs)
        nonexistent_field = 'foo'

        with self.assertRaises(KeyError) as cm:
            coastal_vulnerability.interpolate_sealevelrise_points(
                shore_point_path, slr_point_vector_path,
                nonexistent_field, 'target.pickle')
        actual_message = str(cm.exception)
        expected_message = 'fieldname %s not found in' % nonexistent_field
        self.assertTrue(expected_message in actual_message)

    def test_long_aggregate_radius(self):
        """CV: handle an unreasonably long search radius in raster aggregation."""

        workspace_dir = self.workspace_dir
        raster_path = os.path.join(workspace_dir, 'simple_raster.tif')
        target_pickle_path = os.path.join(workspace_dir, 'target.pickle')
        sample_distance = 1.5

        # Make a simple raster
        srs = osr.SpatialReference()
        srs.ImportFromEPSG(26910)  # UTM Zone 10N
        projection_wkt = srs.ExportToWkt()
        geotransform = [0, 0.5, 0.0, 0, 0.0, -0.5]
        n = 5
        nodata_val = -1
        gtiff_driver = gdal.GetDriverByName('GTiff')
        new_raster = gtiff_driver.Create(
            raster_path, n, n, 1, gdal.GDT_Int32, options=[
                'TILED=YES', 'BIGTIFF=YES', 'COMPRESS=LZW',
                'BLOCKXSIZE=16', 'BLOCKYSIZE=16'])
        new_raster.SetProjection(projection_wkt)
        new_raster.SetGeoTransform(geotransform)
        new_band = new_raster.GetRasterBand(1)
        array = numpy.array(range(n**2)).reshape((n, n))
        new_band.WriteArray(array)
        if nodata_val is not None:
            new_band.SetNoDataValue(nodata_val)
        new_raster.FlushCache()
        new_band = None
        new_raster = None

        # Make a vector proximate to the raster
        simple_points_path = os.path.join(workspace_dir, 'simple_points.shp')
        sampledata.create_vector_on_disk(
            [Point(0.1, -0.1),  # pixel (0,0): kernel origin out of bounds
             Point(1.25, -1.25),  # pixel (2,2): kernel origin & extent out of bounds
             Point(2.1, -2.1),  # pixel (4,4): kernel extent out of bounds
             ],
            srs.ExportToWkt(), filename=simple_points_path)

        coastal_vulnerability._aggregate_raster_values_in_radius(
            simple_points_path, raster_path, sample_distance,
            target_pickle_path, 'mean')

        with open(target_pickle_path, 'rb') as file:
            actual_values = pickle.load(file)

        expected_values = numpy.array([6.5454, 12.0, 17.4545])

        numpy.testing.assert_almost_equal(
            list(actual_values.values()), expected_values, decimal=4)

    def test_complete_run(self):
        """CV: regression test for a complete run with all optional arguments."""

        args = CoastalVulnerabilityTests.generate_base_args(self.workspace_dir)
        # these optional args aren't included in base_args:
        args['geomorphology_vector_path'] = os.path.join(
            INPUT_DATA, 'geomorphology_few_ranks.shp')
        args['geomorphology_fill_value'] = 3
        args['population_raster_path'] = os.path.join(
            INPUT_DATA, 'population.tif')
        args['population_radius'] = 16000
        args['slr_vector_path'] = os.path.join(
            INPUT_DATA, 'sea_level_rise.gpkg')
        args['slr_field'] = 'Trend'
        coastal_vulnerability.execute(args)

        actual_values_df = pandas.read_csv(
            os.path.join(args['workspace_dir'], 'coastal_exposure.csv'))
        expected_values_df = pandas.read_csv(
            os.path.join(REGRESSION_DATA, 'expected_coastal_exposure.csv'))
        pandas.testing.assert_frame_equal(actual_values_df, expected_values_df)

    def test_final_risk_calc(self):
        """CV: regression test for the final risk score calculation."""
        workspace_dir = self.workspace_dir

        target_point_vector_path = os.path.join(
            workspace_dir, 'coastal_exposure.gpkg')
        target_point_csv_path = os.path.join(
            workspace_dir, 'coastal_exposure.csv')

        # Points with ranks for the final equation. Also includes a field
        # without the R_ prefix, which final equation should ignore.
        base_vector_path = os.path.join(REGRESSION_DATA, 'coastal_exposure.gpkg')

        # This input gets modified in place, so first copy to working dir
        # I'm using GPKG driver to copy because that driver may have problems
        # updating a file created by a different GPKG driver version, and the version
        # used is dependent on GDAL version. https://gdal.org/drivers/vector/gpkg.html
        base_shore_point_vector = ogr.Open(base_vector_path)
        gpkg_driver = ogr.GetDriverByName('GPKG')
        gpkg_driver.CopyDataSource(
            base_shore_point_vector, target_point_vector_path)

        coastal_vulnerability.calculate_final_risk(
            target_point_vector_path, target_point_csv_path)

        actual_values_df = pandas.read_csv(target_point_csv_path)
        expected_values_df = pandas.read_csv(
            os.path.join(REGRESSION_DATA, 'expected_final_risk.csv'))
        pandas.testing.assert_frame_equal(actual_values_df, expected_values_df)

    def test_geometric_mean_with_nan(self):
        """CV: test geometric mean function retuns `nan` with missing data."""
        array = numpy.array([1, 1, 1, 1, None], dtype=numpy.float)
        result = coastal_vulnerability._geometric_mean(array)
        assert numpy.isnan(result)

    def test_final_risk_calc_with_missing_data(self):
        """CV: test missing data at feature propogates to empty field in output."""

        target_vector_path = os.path.join(self.workspace_dir, 'target.gpkg')
        target_csv_path = os.path.join(self.workspace_dir, 'target.csv')

        # This gpkg has a feature with an empty field value for 'R_slr'
        # The function modifies the file in place, so copy to test workspace first.

        # I'm using GPKG driver to copy because that driver may have problems
        # updating a file created by a different GPKG driver version, and the version
        # used is dependent on GDAL version. https://gdal.org/drivers/vector/gpkg.html
        base_vector_path = os.path.join(REGRESSION_DATA, 'test_missing_values.gpkg')
        base_shore_point_vector = ogr.Open(base_vector_path)
        gpkg_driver = ogr.GetDriverByName('GPKG')
        gpkg_driver.CopyDataSource(
            base_shore_point_vector, target_vector_path)

        coastal_vulnerability.calculate_final_risk(
            target_vector_path, target_csv_path)
        actual_values_df = pandas.read_csv(target_csv_path)

        # These fields should have missing values after the final calculations
        na_cols = ['exposure', 'habitat_role', 'exposure_no_habitats']
        na_data = [numpy.nan] * 3
        expected_df = pandas.DataFrame([na_data], columns=na_cols)

        pandas.testing.assert_frame_equal(
            actual_values_df[na_cols], expected_df)

    def test_binning_with_missing_data(self):
        """CV: test binning continuous values to ranks, w/ missing values."""
        n = 50
        mask = [10, 20, 30, 40]
        keys = range(n)
        numpy.random.seed(0)
        values = numpy.random.uniform(0, 1, n)
        values[mask] = numpy.nan
        missing_values_dict = dict(zip(keys, values))

        ranks_dict = coastal_vulnerability._bin_values_to_percentiles(
            missing_values_dict)

        # with random uniform values, all 5 ranks should be present
        expected_ranks = set([1, 2, 3, 4, 5])
        assert expected_ranks.issubset(ranks_dict.values())
        # and the masked indices should be nans
        assert all(numpy.isnan(numpy.array(list(ranks_dict.values()))[mask]))

    def test_nodata_raster_aggregation(self):
        """CV: test raster aggregation over entirely nodata returns nan."""
        workspace_dir = self.workspace_dir
        raster_path = os.path.join(workspace_dir, 'nodata_raster.tif')
        target_pickle_path = os.path.join(workspace_dir, 'target.pickle')
        sample_distance = 1.5

        # Make a simple raster filled with all nodata
        srs = osr.SpatialReference()
        srs.ImportFromEPSG(26910)  # UTM Zone 10N
        projection_wkt = srs.ExportToWkt()
        geotransform = [0, 0.5, 0.0, 0, 0.0, -0.5]
        n = 5
        nodata_val = -1
        gtiff_driver = gdal.GetDriverByName('GTiff')
        new_raster = gtiff_driver.Create(
            raster_path, n, n, 1, gdal.GDT_Int32, options=[
                'TILED=YES', 'BIGTIFF=YES', 'COMPRESS=LZW',
                'BLOCKXSIZE=16', 'BLOCKYSIZE=16'])
        new_raster.SetProjection(projection_wkt)
        new_raster.SetGeoTransform(geotransform)
        new_band = new_raster.GetRasterBand(1)
        array = numpy.array([nodata_val] * n**2).reshape((n, n))
        new_band.WriteArray(array)
        if nodata_val is not None:
            new_band.SetNoDataValue(nodata_val)
        new_raster.FlushCache()
        new_band = None
        new_raster = None

        # Make a vector proximate to the raster
        simple_points_path = os.path.join(workspace_dir, 'simple_points.shp')
        sampledata.create_vector_on_disk(
            [Point(0., 0.)],
            srs.ExportToWkt(), filename=simple_points_path)

        coastal_vulnerability._aggregate_raster_values_in_radius(
            simple_points_path, raster_path, sample_distance,
            target_pickle_path, 'density')

        with open(target_pickle_path, 'rb') as file:
            actual_values = pickle.load(file)

        expected_values = numpy.array([numpy.nan])
        numpy.testing.assert_almost_equal(
            list(actual_values.values()), expected_values, decimal=4)

    def test_positive_dem_with_nodata_floats(self):
        """CV: test function that zeros negative DEM values.

        More specifically, test the edge case where the input values are
        floats and very large/small values that overflow an Int16.

        """
        n = 5
        nodata_val = -99999

        array = numpy.array(
            [nodata_val] * n**2, dtype=numpy.float32).reshape((n, n))

        pos_array = coastal_vulnerability.zero_negative_values(
            array, nodata_val)
        # It's all nodata going in, so should be all same nodata out.
        numpy.testing.assert_array_equal(array, pos_array)

    def test_exception_from_validate_polyline(self):
        """CV: raise ValueError on incorrect geometry type during validation.

        shelf_contour_vector_path must be a line geometry, here it's a polygon.
        """
        gpkg_driver = ogr.GetDriverByName("GPKG")
        shelf_poly_path = os.path.join(self.workspace_dir, 'shelf_poly.gpkg')
        vector = gpkg_driver.CreateDataSource(shelf_poly_path)
        vector.CreateLayer(
            'layer', osr.SpatialReference(), ogr.wkbPolygon)
        vector = None
        args = CoastalVulnerabilityTests.generate_base_args(self.workspace_dir)
        args['shelf_contour_vector_path'] = shelf_poly_path
        with self.assertRaises(ValueError):
            err_list = coastal_vulnerability.validate(args)
            for keys, err_strings in err_list:
                if 'Must be a polyline vector' in err_strings:
                    raise ValueError(err_list)

    def test_shore_points_on_single_polygon(self):
        """CV: test shore point creation with single polygon landmass."""
        args = CoastalVulnerabilityTests.generate_base_args(self.workspace_dir)
        aoi_path = os.path.join(self.workspace_dir, 'aoi.geojson')
        srs = osr.SpatialReference()
        srs.ImportFromEPSG(26910)  # UTM Zone 10N
        wkt = srs.ExportToWkt()
        sampledata.create_vector_on_disk(
            [Polygon([(-200, -200), (200, -200), (200, 200), (-200, 200), (-200, -200)])],
            wkt, filename=aoi_path)

        landmass_path = os.path.join(
            self.workspace_dir, 'landmass.geojson')
        sampledata.create_vector_on_disk(
            [Polygon([(-100, -100), (100, -100), (100, 100), (-100, 100), (-100, -100)])],
            wkt, filename=landmass_path)

        args['aoi_vector_path'] = aoi_path
        args['landmass_vector_path'] = landmass_path
        args['model_resolution'] = 100

        polygon_pickle = os.path.join(
            self.workspace_dir, 'polygon.pickle')
        lines_pickle = os.path.join(
            self.workspace_dir, 'lines.pickle')
        lines_rtree = os.path.join(
            self.workspace_dir, 'rtree.dat')
        target_vector_path = os.path.join(
            self.workspace_dir, 'shore_points.gpkg')

        coastal_vulnerability.prepare_landmass_line_index(
            args['landmass_vector_path'], polygon_pickle,
            lines_pickle, lines_rtree)
        coastal_vulnerability.interpolate_shore_points(
            args['aoi_vector_path'], lines_pickle,
            args['model_resolution'], target_vector_path)

        vector = gdal.OpenEx(
            target_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
        layer = vector.GetLayer()
        n_points = layer.GetFeatureCount()
        assert n_points == 8

    def test_shore_points_on_multi_polygon(self):
        """CV: test shore point creation with multipolygon landmass."""

        workspace_dir = self.workspace_dir
        workspace_dir = '/home/dmf/invest_dev/coastal_vulnerability/shore_points_test'

        args = CoastalVulnerabilityTests.generate_base_args(workspace_dir)
        aoi_path = os.path.join(workspace_dir, 'aoi.geojson')
        srs = osr.SpatialReference()
        srs.ImportFromEPSG(26910)  # UTM Zone 10N
        wkt = srs.ExportToWkt()
        sampledata.create_vector_on_disk(
            [Polygon([(-200, -200), (200, -200), (200, 200), (-200, 200), (-200, -200)])],
            wkt, filename=aoi_path)

        landmass_path = os.path.join(
            workspace_dir, 'landmass.geojson')
        poly_a = Polygon([(-200, -200), (-100, -200), (-100, -100), (-200, -100), (-200, -200)])
        poly_b = Polygon([(100, 100), (200, 100), (200, 200), (100, 200), (100, 100)])
        sampledata.create_vector_on_disk(
            [poly_a,
             poly_b,
             MultiPolygon([poly_a, poly_b])],
            wkt, filename=landmass_path)

        args['aoi_vector_path'] = aoi_path
        args['landmass_vector_path'] = landmass_path
        args['model_resolution'] = 100

        polygon_pickle = os.path.join(
            workspace_dir, 'polygon.pickle')
        lines_pickle = os.path.join(
            workspace_dir, 'lines.pickle')
        lines_rtree = os.path.join(
            workspace_dir, 'rtree.dat')
        target_vector_path = os.path.join(
            workspace_dir, 'shore_points.gpkg')

        coastal_vulnerability.prepare_landmass_line_index(
            args['landmass_vector_path'], polygon_pickle,
            lines_pickle, lines_rtree)
        coastal_vulnerability.interpolate_shore_points(
            args['aoi_vector_path'], lines_pickle,
            args['model_resolution'], target_vector_path)

        vector = gdal.OpenEx(
            target_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
        layer = vector.GetLayer()
        n_points = layer.GetFeatureCount()
        assert n_points == 8

    def test_aoi_multiple_features(self):
        """CV: test shore point creation in AOI with multiple features."""

        workspace_dir = self.workspace_dir
        workspace_dir = '/home/dmf/invest_dev/coastal_vulnerability/aoi_polygon_test'

        args = CoastalVulnerabilityTests.generate_base_args(workspace_dir)
        aoi_path = os.path.join(workspace_dir, 'aoi.geojson')
        srs = osr.SpatialReference()
        srs.ImportFromEPSG(26910)  # UTM Zone 10N
        wkt = srs.ExportToWkt()
        poly_a = Polygon([(-200, -200), (-100, -200), (-100, -100), (-200, -100), (-200, -200)])
        poly_b = Polygon([(100, 100), (200, 100), (200, 200), (100, 200), (100, 100)])
        sampledata.create_vector_on_disk(
            [poly_a, poly_b],
            wkt, filename=aoi_path)

        landmass_path = os.path.join(
            workspace_dir, 'landmass.geojson')
        sampledata.create_vector_on_disk(
            [Polygon([(-190, -190), (190, -190), (190, 190), (-190, 190), (-190, -190)])],
            wkt, filename=landmass_path)

        args['aoi_vector_path'] = aoi_path
        args['landmass_vector_path'] = landmass_path
        args['model_resolution'] = 80

        polygon_pickle = os.path.join(
            workspace_dir, 'polygon.pickle')
        lines_pickle = os.path.join(
            workspace_dir, 'lines.pickle')
        lines_rtree = os.path.join(
            workspace_dir, 'rtree.dat')
        target_vector_path = os.path.join(
            workspace_dir, 'shore_points.gpkg')

        coastal_vulnerability.prepare_landmass_line_index(
            args['landmass_vector_path'], polygon_pickle,
            lines_pickle, lines_rtree)
        coastal_vulnerability.interpolate_shore_points(
            args['aoi_vector_path'], lines_pickle,
            args['model_resolution'], target_vector_path)

        vector = gdal.OpenEx(
            target_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
        layer = vector.GetLayer()
        n_points = layer.GetFeatureCount()
        assert n_points == 6

    def test_no_wwiii_coverage(self):
        """CV: test exception when shore points are outside max wwiii distance."""
        args = CoastalVulnerabilityTests.generate_base_args(self.workspace_dir)
        srs = osr.SpatialReference()
        srs.ImportFromEPSG(26910)  # UTM Zone 10N

        simple_points_path = os.path.join(self.workspace_dir, 'simple_points.shp')
        sampledata.create_vector_on_disk(
            [Point(0.0, 0.0)],
            srs.ExportToWkt(), filename=simple_points_path)

        target_path = os.path.join(self.workspace_dir, 'target.gpkg')
        with self.assertRaises(ValueError):
            coastal_vulnerability.interpolate_wwiii_to_shore(
                simple_points_path, args['wwiii_vector_path'],
                target_path)

    def test_prepare_landmass_invalid_geometry(self):
        """CV: test handling invalid geometries in landmass vector."""

        srs = osr.SpatialReference()
        srs.ImportFromEPSG(32731)  # WGS84/UTM zone 31s

        # Invalid geometry examples from: https://github.com/tudelft3d/prepair
        # 3 of these geomtries can be fixed with a 0-width buffer.
        # I don't have an example that CAN be loaded by shapely, but cannot be
        # fixed by the buffer.

        # Make these geometries non-overlapping because the CV function
        # being tested does a cascaded union. Isolating these geometries makes
        # it possible to assert that the number of geometries going in equals
        # the number of features coming out.

        # 1: Bowtie polygon
        invalid_bowtie_polygon = ogr.CreateGeometryFromWkt(
            'POLYGON ((-20 -20, -16 -20, -20 -16, -16 -16, -20 -20))')
        self.assertFalse(invalid_bowtie_polygon.IsValid())

        # 2: Inner ring with one edge sharing part of an edge of the outer ring:
        invalid_shared_edge_polygon = ogr.CreateGeometryFromWkt(
            'POLYGON((0 0, 10 0, 10 10, 0 10, 0 0),(5 2,5 7,10 7, 10 2, 5 2))')
        self.assertFalse(invalid_shared_edge_polygon.IsValid())

        # 3: Dangling edge:
        invalid_dangling_edge_polygon = ogr.CreateGeometryFromWkt(
            'POLYGON((100 100, 110 100, 115 105, 110 100, 110 110, 100 110, 100 100))')
        self.assertFalse(invalid_dangling_edge_polygon.IsValid())

        # One invalid geom that cannot be loaded by shapely or fixed by buffer
        # We expect this polygon to be skipped
        # 4: invalid open ring polygon
        invalid_open_ring_polygon = ogr.CreateGeometryFromWkt(
            'POLYGON ((2 -2, 6 -2, 6 -6, 2 -6))')
        self.assertFalse(invalid_open_ring_polygon.IsValid())

        gpkg_driver = gdal.GetDriverByName('GPKG')
        landmass_vector_path = os.path.join(self.workspace_dir, 'vector.gpkg')
        landmass_vector = gpkg_driver.Create(
            landmass_vector_path, 0, 0, 0, gdal.GDT_Unknown)
        landmass_layer = landmass_vector.CreateLayer(
            'landmass_layer', srs, ogr.wkbUnknown)

        landmass_layer.StartTransaction()
        input_geom_list = [invalid_bowtie_polygon,
                           invalid_shared_edge_polygon,
                           invalid_dangling_edge_polygon,
                           invalid_open_ring_polygon]
        for geometry in input_geom_list:
            outflow_feature = ogr.Feature(landmass_layer.GetLayerDefn())
            outflow_feature.SetGeometry(geometry)
            landmass_layer.CreateFeature(outflow_feature)
        landmass_layer.CommitTransaction()

        landmass_layer = None
        landmass_vector = None

        target_polygon_pickle_path = os.path.join(
            self.workspace_dir, 'polygon.pickle')
        target_lines_pickle_path = os.path.join(
            self.workspace_dir, 'lines.pickle')
        target_rtree_path = os.path.join(self.workspace_dir, 'rtree.dat')
        # Create rtree files to exercise the function's logic of removing
        # pre-exisiting files
        target_rtree_path_base = os.path.splitext(target_rtree_path)[0]
        open(target_rtree_path, 'a').close()
        open(target_rtree_path_base + '.idx', 'a').close()
        coastal_vulnerability.prepare_landmass_line_index(
            landmass_vector_path, target_polygon_pickle_path,
            target_lines_pickle_path, target_rtree_path)

        with open(target_polygon_pickle_path, 'rb') as polygon_file:
            shapely_geom_list = pickle.load(polygon_file)

        # Expect 1 input geometry to be skipped, and the rest to be in
        # shapely_geom_list.
        self.assertTrue(len(shapely_geom_list) == len(input_geom_list) - 1)


def assert_pickled_arrays_almost_equal(
        actual_values_pickle_path, expected_values_json_path):
    """Open a pickled dict and assert keys and values match expected.

    Expected data is stored in json as pickles are really only reliably
    read by the same program that wrote them.
    """
    with open(actual_values_pickle_path, 'rb') as pickle_file:
        actual_values_dict = pickle.load(pickle_file)
    actual_values = list(actual_values_dict.values())
    actual_fids = list(actual_values_dict.keys())

    with open(expected_values_json_path, 'r') as json_file:
        expected_values_dict = json.load(json_file)
    expected_values_dict = {
        int(x[0]): float(x[1]) for x in expected_values_dict.items()}
    # the dict items need sorting by FID to match the pre-sorted pickled items
    expected_fids = [x[0] for x in sorted(expected_values_dict.items())]
    expected_values = [x[1] for x in sorted(expected_values_dict.items())]

    numpy.testing.assert_array_almost_equal(
        actual_values, expected_values, decimal=2)
    numpy.testing.assert_array_equal(actual_fids, expected_fids)

def make_slr_vector(slr_point_vector_path, fieldname, shapely_feature, srs):
    """Make an SLR point very far away"""
    driver = ogr.GetDriverByName('ESRI Shapefile')
    out_vector = driver.CreateDataSource(slr_point_vector_path)
    layer_name = os.path.basename(os.path.splitext(slr_point_vector_path)[0])
    out_layer = out_vector.CreateLayer(layer_name, srs=srs)
    field_defn = ogr.FieldDefn(fieldname, ogr.OFTReal)
    out_layer.CreateField(field_defn)
    layer_defn = out_layer.GetLayerDefn()
    new_feature = ogr.Feature(layer_defn)
    new_geometry = ogr.CreateGeometryFromWkb(shapely_feature.wkb)
    new_feature.SetGeometry(new_geometry)
    new_feature.SetField(fieldname, 1.3)  # any value will do
    out_layer.CreateFeature(new_feature)
    out_layer = None
    out_vector = None