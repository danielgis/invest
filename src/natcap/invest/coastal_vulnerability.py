"""InVEST Coastal Vulnerability"""
import sys
import traceback
import shutil
import time
import os
import math
import logging
import re
import multiprocessing
import zipfile
import pickle
import tempfile
import json

import numpy
from osgeo import gdal
from osgeo import osr
from osgeo import ogr
import pandas
import rtree
import shapely
import shapely.wkb
import shapely.ops
import shapely.speedups
import pygeoprocessing
import taskgraph

from . import utils

LOGGER = logging.getLogger(__name__)

# Wave Watch III data does not cover the planet.  Make sure we don't deal
# with a point that's not in range of said point.  I'm picking 1 degree since
# that's double the diagonal distance between two WWIII points
_MAX_WWIII_DISTANCE = 5.0  # RICH: did you later decide 5 degree is better choice than 1?
_N_FETCH_RAYS = 16


# Create shore points within an AOI 
# Clip the landmass vector path by an AOI
# Could pass AOI to create_shore_points and clip landmass (w/ landmass_rtree)
# Or could clip landmass ahead of time and pass clipped landmass to create_shore_points


# just pass aoi vector to create shore points, for create_raster_from_vector_extents
# then rasterize landmass onto that raster. still need to make sure landmass
# is projected to AOI. Then, wind exposure needs a landmass in AOI projection 
# with a buffer of max-fetch-dist. 

def execute(args):
    """
    
    Parameters:
        workspace_dir (string):
        n_workers:
        wwiii:
        landmass_vector_path:
        aoi_vector_path:
        smallest_feature_size:
        max_fetch_distance:


    """

    output_dir = os.path.join(args['workspace_dir'])
    intermediate_dir = os.path.join(
        args['workspace_dir'], 'intermediate')

    utils.make_directories(
        [output_dir, intermediate_dir])

    work_token_dir = os.path.join(intermediate_dir, '_tmp_work_tokens')
    try:
        n_workers = int(args['n_workers'])
    except (KeyError, ValueError, TypeError):
        # KeyError when n_workers is not present in args
        # ValueError when n_workers is an empty string.
        # TypeError when n_workers is None.
        n_workers = -1  # Single process mode.
    task_graph = taskgraph.TaskGraph(work_token_dir, n_workers)

    # TODO: reproject global inputs (wwiii, global landmass) to AOI SRS, 
    # Before building rtree?
    # Or, build rtrees for global data, then reproject AOI to match,
    # AOI should be small/simple.
    # - wwiii: I project points to match wwiii, for finding nearest.
    # - landmass: project aoi to landmass, clip landmass, then prj land to aoi
    # - relief/bathy: 
    # - continental shelf: 

    wwiii_rtree_path = os.path.join(intermediate_dir, 'wwiii_rtree.dat')
    build_wwiii_rtree_task = task_graph.add_task(
        func=build_wwiii_rtree,
        args=(args['wwiii'], wwiii_rtree_path),
        target_path_list=[wwiii_rtree_path],
        task_name='build_wwiii_rtree')

    landmass_rtree_path = os.path.join(intermediate_dir, 'landmass_rtree.dat')
    build_landmass_rtree_task = task_graph.add_task(
        func=build_feature_bounding_box_rtree,
        args=(args['landmass_vector_path'], landmass_rtree_path),
        target_path_list=[landmass_rtree_path],
        task_name='build_landmass_rtree')

    clipped_landmass_path = os.path.join(
        intermediate_dir, 'clipped_landmass.gpkg')
    clip_landmass_to_aoi_task = task_graph.add_task(
        func=clip_reproject_landmass_to_aoi,
        args=(args['aoi_vector_path'], args['landmass_vector_path'],
              landmass_rtree_path, args['max_fetch_distance'],
              args['smallest_feature_size'], clipped_landmass_path),
        target_path_list=[clipped_landmass_path],
        dependent_task_list=[build_landmass_rtree_task],
        task_name='clip landmass to aoi')

    shore_point_vector_path = os.path.join(
        output_dir, 'shore_points.gpkg')
    create_shore_points_task = task_graph.add_task(
        func=create_shore_points,
        args=(args['aoi_vector_path'], clipped_landmass_path,
              args['smallest_feature_size'], intermediate_dir,
              shore_point_vector_path),
        target_path_list=[shore_point_vector_path],
        dependent_task_list=[clip_landmass_to_aoi_task, build_wwiii_rtree_task],
        task_name='create shore points')

    exposure_variables_task_list = []
    exposure_variables_path_list = []

    target_wwiii_point_vector_path = os.path.join(
        intermediate_dir, 'wwiii_shore_points.gpkg')
    interpolate_wwiii_task = task_graph.add_task(
        func=interpolate_wwiii_to_shore,
        args=(shore_point_vector_path, args['aoi_vector_path'], args['wwiii'],
              wwiii_rtree_path, target_wwiii_point_vector_path),
        target_path_list=[target_wwiii_point_vector_path],
        dependent_task_list=[create_shore_points_task],
        task_name='interpolate wwiii to shore points')
    exposure_variables_task_list.append(interpolate_wwiii_task)

    fetch_point_vector_path = os.path.join(
        intermediate_dir, 'fetch_points.gpkg')
    target_wind_exposure_pickle_path = os.path.join(
        intermediate_dir, 'wind.pickle')
    exposure_variables_path_list.append(target_wind_exposure_pickle_path)
    wind_exposure_task = task_graph.add_task(
        func=calculate_wind_exposure,
        args=(target_wwiii_point_vector_path, landmass_rtree_path,
              clipped_landmass_path, intermediate_dir,
              args['smallest_feature_size'],
              args['max_fetch_distance'], fetch_point_vector_path,
              target_wind_exposure_pickle_path),
        target_path_list=[
            fetch_point_vector_path, target_wind_exposure_pickle_path],
        dependent_task_list=[interpolate_wwiii_task],
        task_name='calculate wind exposure')
    exposure_variables_task_list.append(wind_exposure_task)

    target_wave_exposure_path = os.path.join(
        intermediate_dir, 'wave.pickle')
    exposure_variables_path_list.append(target_wave_exposure_path)
    exposure_variables_task_list.append(task_graph.add_task(
        func=calculate_wave_exposure,
        args=(fetch_point_vector_path, args['max_fetch_distance'],
              target_wave_exposure_path),
        target_path_list=[target_wave_exposure_path],
        dependent_task_list=[wind_exposure_task],
        task_name='calculate wave exposure'))

    target_surge_exposure_path = os.path.join(
        intermediate_dir, 'surge.pickle')
    exposure_variables_path_list.append(target_surge_exposure_path)
    exposure_variables_task_list.append(task_graph.add_task(
        func=calculate_surge_exposure,
        args=(shore_point_vector_path, args['shelf_contour_path'],
              args['aoi_vector_path'], intermediate_dir,
              target_surge_exposure_path),
        target_path_list=[target_surge_exposure_path],
        dependent_task_list=[create_shore_points_task],
        task_name='calculate surge exposure'))

    relief_point_pickle_path = os.path.join(
        intermediate_dir, 'relief.pickle')
    exposure_variables_path_list.append(relief_point_pickle_path)
    exposure_variables_task_list.append(task_graph.add_task(
        func=calculate_relief_exposure,
        args=(shore_point_vector_path, args['dem'],
              args['dem_averaging_radius'],
              args['smallest_feature_size'], intermediate_dir,
              relief_point_pickle_path),
        target_path_list=[relief_point_pickle_path],
        dependent_task_list=[create_shore_points_task],
        task_name='calculate relief exposure'))

    target_habitat_protection_path = os.path.join(
        output_dir, 'habitat_protection.csv')
    exposure_variables_task_list.append(task_graph.add_task(
        func=calculate_habitat_protection,
        args=(shore_point_vector_path,
              args['habitat_table_path'], intermediate_dir,
              target_habitat_protection_path),
        target_path_list=[target_habitat_protection_path],
        dependent_task_list=[create_shore_points_task],
        task_name='calculate habitat protection'))

    # TODO: i'm creating this grid_raster_path in two places now.
    grid_raster_path = os.path.join(intermediate_dir, 'grid.tif')
    target_geomorphology_raster_path = os.path.join(
        intermediate_dir, 'final_geomorphology_rank.tif')
    target_geomorphology_pickle_path = os.path.join(
        intermediate_dir, 'geomorph.pickle')
    exposure_variables_path_list.append(target_geomorphology_pickle_path)
    exposure_variables_task_list.append(task_graph.add_task(
        func=calculate_geomorphology_exposure,
        args=(args['geomorphology_vector_path'],
              grid_raster_path, intermediate_dir,
              target_geomorphology_raster_path,
              shore_point_vector_path,
              target_geomorphology_pickle_path),
        target_path_list=[
            target_geomorphology_raster_path, target_geomorphology_pickle_path],
        dependent_task_list=[create_shore_points_task],
        task_name='calculate geomorphology exposure'))

    target_final_risk_vector_path = os.path.join(
        output_dir, 'coastal_exposure.gpkg')
    task_graph.add_task(
        func=calculate_final_risk,
        args=(exposure_variables_path_list, target_habitat_protection_path,
              shore_point_vector_path, target_final_risk_vector_path),
        target_path_list=[target_final_risk_vector_path],
        dependent_task_list=exposure_variables_task_list,
        task_name='calculate final risk')

    task_graph.close()
    task_graph.join()


def clip_reproject_landmass_to_aoi(
        aoi_vector_path, landmass_vector_path, landmass_rtree_path,
        max_fetch_distance, smallest_feature_size,
        target_clipped_landmass_path):
    """Clip landmass polygon to AOI.

    # TODO: generalize this function so it just clips to a bbox that is passed in?
    """
    LOGGER.info("clipping landmass to AOI extent")
    landmass_vector_rtree = rtree.index.Index(
        os.path.splitext(landmass_rtree_path)[0])
    landmass_spatial_reference = osr.SpatialReference()
    landmass_srs_wkt = pygeoprocessing.get_vector_info(
        landmass_vector_path)['projection']
    landmass_spatial_reference.ImportFromWkt(landmass_srs_wkt)
    landmass_vector = gdal.OpenEx(landmass_vector_path, gdal.OF_VECTOR)
    landmass_layer = landmass_vector.GetLayer()

    aoi_vector_info = pygeoprocessing.get_vector_info(
        aoi_vector_path)
    aoi_srs_wkt = aoi_vector_info['projection']
    aoi_spatial_reference = osr.SpatialReference()
    aoi_spatial_reference.ImportFromWkt(aoi_srs_wkt)
    aoi_bounding_box = aoi_vector_info['bounding_box']
    # add the max_fetch_distance to the bounding box so we can use
    # this clipped landmass in the ray casting routine.
    fetch_buffer = max_fetch_distance + smallest_feature_size
    aoi_bounding_box[0] -= fetch_buffer
    aoi_bounding_box[1] -= fetch_buffer
    aoi_bounding_box[2] += fetch_buffer
    aoi_bounding_box[3] += fetch_buffer

    aoi_clipping_box = pygeoprocessing.transform_bounding_box(
        aoi_bounding_box, aoi_srs_wkt,
        landmass_srs_wkt, edge_samples=11)
    aoi_clipping_shapely = shapely.geometry.box(*aoi_clipping_box)

    landmass_to_aoi_transform = osr.CoordinateTransformation(
        landmass_spatial_reference, aoi_spatial_reference)

    gpkg_driver = gdal.GetDriverByName('GPKG')
    clipped_vector = gpkg_driver.Create(
        target_clipped_landmass_path, 0, 0, 0, gdal.GDT_Unknown)
    clipped_layer = clipped_vector.CreateLayer(
        os.path.splitext(os.path.basename(target_clipped_landmass_path))[0],
        aoi_spatial_reference, ogr.wkbPolygon)
    clipped_defn = clipped_layer.GetLayerDefn()

    for feature_id in landmass_vector_rtree.intersection(
            aoi_clipping_box):
        try:
            landmass_feature = landmass_layer.GetFeature(feature_id)
            landmass_shapely = shapely.wkb.loads(
                landmass_feature.GetGeometryRef().ExportToWkb())
            intersection_shapely = aoi_clipping_shapely.intersection(
                landmass_shapely)
            clipped_geometry = ogr.CreateGeometryFromWkt(
                intersection_shapely.wkt)
            err_code = clipped_geometry.Transform(landmass_to_aoi_transform)
            if err_code != 0:
                raise ValueError(
                    "Could not project %s to spatial reference system of %s"
                    % (landmass_vector_path, aoi_vector_path))
            clipped_feature = ogr.Feature(clipped_defn)
            clipped_feature.SetGeometry(clipped_geometry)
            clipped_layer.CreateFeature(clipped_feature)
            clipped_feature = None
        except Exception:
            clipped_feature = None
            LOGGER.warn(
                "Couldn't process this intersection %s",
                intersection_shapely)
    clipped_layer.SyncToDisk()
    clipped_layer = None
    clipped_vector = None


def create_shore_points(
        aoi_vector_path, landmass_vector_path, smallest_feature_size,
        workspace_dir, target_shore_point_vector_path):
    """Create points that lie on the coast line of the landmass.

    Parameters:
        landmass_vector_path (string): path to polygon vector representing
            landmass.
        smallest_feature_size (float): smallest feature size to grid a shore
            point on.
        workspace_dir (string): path to a directory that can be created
            during run to hold temporary files.  Will be deleted on successful
            function completion.
        target_shore_point_vector_path (string): path to a point vector that
            will be created and contain points on the shore of the landmass.

    Returns:
        None.

    """
    LOGGER.info("creating shore points along edge of landmass")
    # create the spatial reference from the base vector
    aoi_vector_info = pygeoprocessing.get_vector_info(aoi_vector_path)
    aoi_spatial_reference = osr.SpatialReference()
    aoi_spatial_reference.ImportFromWkt(aoi_vector_info['projection'])
    aoi_bounding_box = aoi_vector_info['bounding_box']

    grid_raster_path = os.path.join(workspace_dir, 'grid.tif')
    land_mask_raster_path = os.path.join(workspace_dir, 'land_mask.tif')
    convolution_raster_path = os.path.join(
        workspace_dir, 'convolution.tif')
    masked_convolution_raster_path = os.path.join(
        workspace_dir, 'masked_convolution.tif')
    shore_kernel_path = os.path.join(
        workspace_dir, 'shore_kernel.tif')
    shore_raster_path = os.path.join(
        workspace_dir, 'shore_raster.tif')

    gpkg_driver = gdal.GetDriverByName("GPKG")

    # this will hold the output sample points on the shore
    target_shore_point_vector = gpkg_driver.Create(
        target_shore_point_vector_path, 0, 0, 0, gdal.GDT_Unknown)
    target_shore_point_layer = target_shore_point_vector.CreateLayer(
        os.path.basename(os.path.splitext(target_shore_point_vector_path)[0]),
        aoi_spatial_reference, ogr.wkbPoint)

    target_shore_point_defn = target_shore_point_layer.GetLayerDefn()

    byte_nodata = 255
    # Create a raster from the AOI extent, plus one pixel in all directions
    # add a pixel buffer so we clip land that's a little outside the grid
    pixel_buffer = 1
    aoi_bounding_box[0] -= pixel_buffer * smallest_feature_size
    aoi_bounding_box[1] -= pixel_buffer * smallest_feature_size
    aoi_bounding_box[2] += pixel_buffer * smallest_feature_size
    aoi_bounding_box[3] += pixel_buffer * smallest_feature_size

    # round up on the rows and cols so that the target raster encloses the
    # base vector
    target_pixel_size = (
        smallest_feature_size / 2.0, -smallest_feature_size / 2.0)  # TODO: why divide by 2?
    n_cols = int(numpy.ceil(
        abs((aoi_bounding_box[2] - aoi_bounding_box[0]) / target_pixel_size[0])))
    n_rows = int(numpy.ceil(
        abs((aoi_bounding_box[3] - aoi_bounding_box[1]) / target_pixel_size[1])))

    driver = gdal.GetDriverByName('GTiff')
    n_bands = 1
    template_raster = driver.Create(
        grid_raster_path, n_cols, n_rows, n_bands, gdal.GDT_Byte)
    template_raster.GetRasterBand(1).SetNoDataValue(byte_nodata)

    # Set the transform based on the upper left corner and given pixel
    # dimensions
    x_source = aoi_bounding_box[0]
    y_source = aoi_bounding_box[3]
    raster_transform = [
        x_source, target_pixel_size[0], 0.0,
        y_source, 0.0, target_pixel_size[1]]
    template_raster.SetGeoTransform(raster_transform)
    template_raster.SetProjection(aoi_spatial_reference.ExportToWkt())

    # Initialize everything to 0
    band = template_raster.GetRasterBand(1)
    band.Fill(0)
    band.FlushCache()
    band = None

    # Make a copy before rasterizing landmass
    # because the all 0s raster is useful for the geomorphology routine
    driver.CreateCopy(land_mask_raster_path, template_raster)
    template_raster = None

    # rasterize landmass to grid
    pygeoprocessing.rasterize(
        landmass_vector_path, land_mask_raster_path, [1], None)

    # grid shoreline from raster
    make_shore_kernel(shore_kernel_path)
    pygeoprocessing.convolve_2d(
        (land_mask_raster_path, 1), (shore_kernel_path, 1),
        convolution_raster_path, target_datatype=gdal.GDT_Byte,
        target_nodata=255)

    # Apply mask to trim off edges of the convolution, which
    # incorrectly categorize as shore.
    pygeoprocessing.mask_raster(
        (convolution_raster_path, 1), aoi_vector_path,
        masked_convolution_raster_path, all_touched=False)

    temp_grid_nodata = pygeoprocessing.get_raster_info(
        land_mask_raster_path)['nodata'][0]

    def _shore_mask_op(shore_convolution):
        """Mask values on land that border water."""
        result = numpy.empty(shore_convolution.shape, dtype=numpy.uint8)
        result[:] = byte_nodata
        valid_mask = shore_convolution != temp_grid_nodata
        # If a pixel is on land, it gets at least a 9, but if it's all on
        # land it gets an 17 (8 neighboring pixels), so we search between 9
        # and 17 to determine a shore pixel
        result[valid_mask] = numpy.where(
            (shore_convolution[valid_mask] >= 9) &
            (shore_convolution[valid_mask] < 17), 1, byte_nodata)
        return result

    pygeoprocessing.raster_calculator(
        [(masked_convolution_raster_path, 1)], _shore_mask_op,
        shore_raster_path, gdal.GDT_Byte, byte_nodata)

    shore_geotransform = pygeoprocessing.get_raster_info(
        shore_raster_path)['geotransform']

    for offset_info, data_block in pygeoprocessing.iterblocks(
            (shore_raster_path, 1)):
        row_indexes, col_indexes = numpy.mgrid[
            offset_info['yoff']:offset_info['yoff']+offset_info['win_ysize'],
            offset_info['xoff']:offset_info['xoff']+offset_info['win_xsize']]
        valid_mask = data_block == 1
        x_coordinates = (
            shore_geotransform[0] +
            shore_geotransform[1] * (col_indexes[valid_mask] + 0.5) +
            shore_geotransform[2] * (row_indexes[valid_mask] + 0.5))
        y_coordinates = (
            shore_geotransform[3] +
            shore_geotransform[4] * (col_indexes[valid_mask] + 0.5) +
            shore_geotransform[5] * (row_indexes[valid_mask] + 0.5))

        for x_coord, y_coord in zip(x_coordinates, y_coordinates):
            # Set the point geometry in the native SRS
            shore_point_geometry = ogr.Geometry(ogr.wkbPoint)
            shore_point_geometry.AddPoint(x_coord, y_coord)
            shore_point_feature = ogr.Feature(target_shore_point_defn)
            shore_point_feature.SetGeometry(shore_point_geometry)

            target_shore_point_layer.CreateFeature(shore_point_feature)
            shore_point_feature = None
    target_shore_point_defn = None
    target_shore_point_layer = None
    target_shore_point_vector = None


def interpolate_wwiii_to_shore(
        base_shore_point_vector_path, aoi_vector_path, wwiii_vector_path,
        wwiii_rtree_path, target_wave_exposure_path):
    """Calculate wave exposure for each shore point.
    Join tabular data from Wave Watch 3 to shore points by finding the
    nearest WW3 points to each shore point.

    Parameters:
        wwiii_vector_path (string): path to point shapefile representing
            the Wave Watch III data.
        wwiii_rtree_path (string): path to an rtree index that has
            the points of `wwiii_vector_path` indexed.
    """
    # Copy shore point geometry and create fields for WWIII values
    base_vector = gdal.OpenEx(
        base_shore_point_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    gpkg_driver = gdal.GetDriverByName("GPKG")
    if os.path.exists(target_wave_exposure_path):
        os.remove(target_wave_exposure_path)
    gpkg_driver.CreateCopy(target_wave_exposure_path, base_vector)
    base_vector = None
    points_vector = gdal.OpenEx(
        target_wave_exposure_path, gdal.OF_VECTOR | gdal.GA_Update)
    layer_name = points_vector.GetLayer().GetName()
    new_layer_name = os.path.basename(
        os.path.splitext(target_wave_exposure_path)[0])
    points_vector.ExecuteSQL(
        "ALTER TABLE %s RENAME TO %s" % (layer_name, new_layer_name))
    points_layer = points_vector.GetLayer()
    wwiii_vector = gdal.OpenEx(
        wwiii_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    wwiii_layer = wwiii_vector.GetLayer()
    wwiii_defn = wwiii_layer.GetLayerDefn()
    field_names = []
    for field_index in range(wwiii_defn.GetFieldCount()):
        field_defn = wwiii_defn.GetFieldDefn(field_index)
        field_name = field_defn.GetName()
        if field_name in ['I', 'J']:
            continue
        field_names.append(field_name)
        points_layer.CreateField(field_defn)

    wwiii_spatial_reference = osr.SpatialReference()
    wwiii_ref_wkt = pygeoprocessing.get_vector_info(
        wwiii_vector_path)['projection']
    wwiii_spatial_reference.ImportFromWkt(wwiii_ref_wkt)

    points_spatial_reference = osr.SpatialReference()
    points_ref_wkt = pygeoprocessing.get_vector_info(
        target_wave_exposure_path)['projection']
    points_spatial_reference.ImportFromWkt(points_ref_wkt)
    points_to_wwiii_transform = osr.CoordinateTransformation(
        points_spatial_reference, wwiii_spatial_reference)

    # rtree index loads without the extension
    wwiii_rtree_base_path = os.path.splitext(
        wwiii_rtree_path)[0]
    wwiii_rtree = rtree.index.Index(wwiii_rtree_base_path)
    wwiii_field_lookup = {}

    # get AOI extent so we only query wwiii points within a reasonable range
    aoi_extent = pygeoprocessing.get_vector_info(aoi_vector_path)['bounding_box']
    aoi_extent_ll = pygeoprocessing.transform_bounding_box(
        aoi_extent, points_ref_wkt, wwiii_ref_wkt, edge_samples=11)

    LOGGER.info("Interpolating shore points with Wave Watch III data")
    for shore_point_feature in points_layer:
        shore_point_geometry = shore_point_feature.GetGeometryRef()
        
        # Transform each point to match the wwiii SRS
        shore_point_longitude, shore_point_latitude, _ = (
            points_to_wwiii_transform.TransformPoint(
                shore_point_geometry.GetX(), shore_point_geometry.GetY()))
        # get the nearest wave watch III points from the shore point
        nearest_points = list(wwiii_rtree.nearest(
            (shore_point_longitude, shore_point_latitude,
             shore_point_longitude, shore_point_latitude), 3))[0:3]

        # create placeholders for point geometry and field values
        wwiii_points = numpy.empty((3, 2))
        wwiii_values = numpy.empty((3, len(field_names)))
        for fid_index, fid in enumerate(nearest_points):
            wwiii_feature = wwiii_layer.GetFeature(fid)
            wwiii_geometry = wwiii_feature.GetGeometryRef()
            wwiii_points[fid_index] = numpy.array(
                [wwiii_geometry.GetX(), wwiii_geometry.GetY()])
            try:
                wwiii_values[fid_index] = wwiii_field_lookup[fid]
            except KeyError:
                wwiii_field_lookup[fid] = numpy.array(
                    [float(wwiii_feature.GetField(field_name))
                     for field_name in field_names])
                wwiii_values[fid_index] = wwiii_field_lookup[fid]
        # distance = numpy.linalg.norm(
        #     wwiii_points - numpy.array(
        #         (shore_point_geometry.GetX(),
        #          shore_point_geometry.GetY())))
        distance = numpy.linalg.norm(
            wwiii_points - numpy.array(
                (shore_point_longitude,
                 shore_point_latitude)))

        # make sure we're within a valid data distance
        if distance > _MAX_WWIII_DISTANCE:
            LOGGER.warning("No WWIII data within range of shore point")
            continue

        wwiii_values *= distance
        wwiii_values = numpy.mean(wwiii_values, axis=0)
        wwiii_values /= numpy.sum(distance)

        for field_name_index, field_name in enumerate(field_names):
            shore_point_feature.SetField(
                field_name, wwiii_values[field_name_index])
        points_layer.SetFeature(shore_point_feature)
        shore_point_feature = None

    points_layer = None
    points_vector = None
    # LOGGER.info("All done with shore points for grid %s", grid_fid)


def calculate_wind_exposure(
        base_shore_point_vector_path,
        landmass_bounding_rtree_path, landmass_vector_path, workspace_dir,
        smallest_feature_size, max_fetch_distance,
        target_fetch_point_vector_path,
        target_wind_exposure_pickle_path):
    """Calculate wind exposure for each shore point.

    Parameters:
        base_shore_point_vector_path (string): path to a point shapefile
            representing shore points that should be sampled for wind
            exposure.
        landmass_bounding_rtree_path (string): path to an rtree bounding box
            for the landmass polygons.
        landmass_vector_path (string): path to landmass polygon vetor.
        workspace_dir (string): path to a directory that can be created for
            temporary workspace files
        smallest_feature_size (float): smallest feature size to detect in
            meters.
        max_fetch_distance (float): maximum fetch distance for a ray in
            meters.
        target_fetch_point_vector_path (string): path to target point file,
            will be a copy of `base_shore_point_vector_path`'s geometry with
            an 'REI' (relative exposure index) field added.

    Returns:
        None

    """
    LOGGER.info("Calculating wind exposure")
    temp_fetch_rays_path = os.path.join(
        workspace_dir, 'fetch_rays.gpkg')

    # this should still match the user-defined SRS from the AOI:
    base_ref_wkt = pygeoprocessing.get_vector_info(
        base_shore_point_vector_path)['projection']
    base_spatial_reference = osr.SpatialReference()
    base_spatial_reference.ImportFromWkt(base_ref_wkt)

    gpkg_driver = gdal.GetDriverByName('GPKG')

    clipped_geometry_shapely_list = []
    temp_utm_clipped_vector = ogr.Open(landmass_vector_path)
    temp_utm_clipped_layer = temp_utm_clipped_vector.GetLayer()
    for tmp_utm_feature in temp_utm_clipped_layer:
        tmp_utm_geometry = tmp_utm_feature.GetGeometryRef()
        shapely_geometry = shapely.wkb.loads(
            tmp_utm_geometry.ExportToWkb())
        if shapely_geometry.is_valid:
            # TODO: if geom is invalid we are just skipping it?
            clipped_geometry_shapely_list.append(shapely_geometry)
        tmp_utm_geometry = None
    temp_utm_clipped_layer = None
    temp_utm_clipped_vector = None
    landmass_shapely = shapely.ops.cascaded_union(
        clipped_geometry_shapely_list)
    clipped_geometry_shapely_list = None

    # load land geometry into shapely object
    landmass_shapely_prep = shapely.prepared.prep(landmass_shapely)

    # explode landmass into lines for easy intersection
    temp_polygon_segements_path = os.path.join(
        workspace_dir, 'polygon_segments.gpkg')
    
    temp_polygon_segments_vector = gpkg_driver.Create(
        temp_polygon_segements_path, 0, 0, 0, gdal.GDT_Unknown)
    temp_polygon_segments_layer = (
        temp_polygon_segments_vector.CreateLayer(
            os.path.splitext(os.path.basename(temp_polygon_segements_path))[0],
            base_spatial_reference, ogr.wkbLineString))
    temp_polygon_segments_defn = temp_polygon_segments_layer.GetLayerDefn()

    polygon_line_rtree = rtree.index.Index()
    polygon_line_index = []
    shapely_line_index = []
    line_id = 0
    LOGGER.info("indexing geometry of landmass")
    for line in geometry_to_lines(landmass_shapely):
        segment_feature = ogr.Feature(temp_polygon_segments_defn)
        segement_geometry = ogr.Geometry(ogr.wkbLineString)
        segement_geometry.AddPoint(*line.coords[0])
        segement_geometry.AddPoint(*line.coords[1])
        segment_feature.SetGeometry(segement_geometry)
        temp_polygon_segments_layer.CreateFeature(segment_feature)

        if (line.bounds[0] == line.bounds[2] and
                line.bounds[1] == line.bounds[3]):
            continue
        polygon_line_rtree.insert(line_id, line.bounds)
        line_id += 1
        polygon_line_index.append(segement_geometry)
        shapely_line_index.append(shapely.wkb.loads(
            segement_geometry.ExportToWkb()))

    temp_polygon_segments_layer.SyncToDisk()
    temp_polygon_segments_layer = None
    temp_polygon_segments_vector = None

    # create fetch rays
    temp_fetch_rays_vector = gpkg_driver.Create(
        temp_fetch_rays_path, 0, 0, 0, gdal.GDT_Unknown)
    temp_fetch_rays_layer = (
        temp_fetch_rays_vector.CreateLayer(
            os.path.splitext(os.path.basename(temp_fetch_rays_path))[0],
            base_spatial_reference, ogr.wkbLineString))
    temp_fetch_rays_defn = temp_fetch_rays_layer.GetLayerDefn()
    temp_fetch_rays_layer.CreateField(ogr.FieldDefn(
        'fetch_dist', ogr.OFTReal))

    # TODO (maybe): we don't need all the fields copied, but we do need some.
    base_shore_point_vector = gdal.OpenEx(
        base_shore_point_vector_path, gdal.OF_VECTOR)
    gpkg_driver.CreateCopy(
        target_fetch_point_vector_path, base_shore_point_vector)
    target_shore_point_vector = gdal.OpenEx(
        target_fetch_point_vector_path, gdal.OF_VECTOR | gdal.GA_Update)
    layer_name = target_shore_point_vector.GetLayer().GetName()
    new_layer_name = os.path.basename(
        os.path.splitext(target_fetch_point_vector_path)[0])
    target_shore_point_vector.ExecuteSQL(
        "ALTER TABLE %s RENAME TO %s" % (layer_name, new_layer_name))
    target_shore_point_layer = target_shore_point_vector.GetLayer()
    target_shore_point_layer.CreateField(
        ogr.FieldDefn('REI', ogr.OFTReal))
    for ray_index in range(_N_FETCH_RAYS):
        compass_degree = int(ray_index * 360 / 16.)
        target_shore_point_layer.CreateField(
            ogr.FieldDefn('fdist_%d' % compass_degree, ogr.OFTReal))

    shore_point_logger = _make_logger_callback(
        "Wind exposure %.2f%% complete.", LOGGER)
    # Iterate over every shore point
    LOGGER.info("Casting rays ")
    result_REI = {}
    for shore_point_feature in target_shore_point_layer:
        shore_point_fid = shore_point_feature.GetFID()
        shore_point_logger(
            float(shore_point_fid) /
            target_shore_point_layer.GetFeatureCount())
        rei_value = 0.0
        # Iterate over every ray direction
        for sample_index in range(_N_FETCH_RAYS):
            compass_degree = int(sample_index * 360 / 16.)
            compass_theta = float(sample_index) / _N_FETCH_RAYS * 360
            rei_pct = shore_point_feature.GetField(
                'REI_PCT%d' % int(compass_theta))
            rei_v = shore_point_feature.GetField(
                'REI_V%d' % int(compass_theta))
            cartesian_theta = -(compass_theta - 90)

            # Determine the direction the ray will point
            delta_x = math.cos(cartesian_theta * math.pi / 180)
            delta_y = math.sin(cartesian_theta * math.pi / 180)

            shore_point_geometry = shore_point_feature.GetGeometryRef()
            point_a_x = (
                shore_point_geometry.GetX() + delta_x * smallest_feature_size)
            point_a_y = (
                shore_point_geometry.GetY() + delta_y * smallest_feature_size)
            point_b_x = point_a_x + delta_x * (
                max_fetch_distance - smallest_feature_size)
            point_b_y = point_a_y + delta_y * (
                max_fetch_distance - smallest_feature_size)
            shore_point_geometry = None

            # build ray geometry so we can intersect it later
            ray_geometry = ogr.Geometry(ogr.wkbLineString)
            ray_geometry.AddPoint(point_a_x, point_a_y)
            ray_geometry.AddPoint(point_b_x, point_b_y)

            # keep a shapely version of the ray so we can do fast intersection
            # with it and the entire landmass
            ray_point_origin_shapely = shapely.geometry.Point(
                point_a_x, point_a_y)

            ray_length = 0.0
            if not landmass_shapely_prep.intersects(
                    ray_point_origin_shapely):
                # the origin is in ocean

                # This algorithm searches for intersections, if one is found
                # the ray updates and a smaller intersection set is determined
                # by experimentation I've found this is significant, but not
                # an order of magnitude, faster than looping through all
                # original possible intersections.  Since this algorithm
                # will be run for a long time, it's worth the additional
                # complexity
                tested_indexes = set()
                while True:
                    intersection = False
                    ray_envelope = ray_geometry.GetEnvelope()
                    for poly_line_index in polygon_line_rtree.intersection(
                            [ray_envelope[i] for i in [0, 2, 1, 3]]):
                        if poly_line_index in tested_indexes:
                            continue
                        tested_indexes.add(poly_line_index)
                        line_segment = (
                            polygon_line_index[poly_line_index])
                        if ray_geometry.Intersects(line_segment):
                            # if the ray intersects the poly line, test if
                            # the intersection is closer than any known
                            # intersection so far
                            intersection_point = ray_geometry.Intersection(
                                line_segment)
                            # offset the dist with smallest_feature_size
                            # update the endpoint of the ray
                            ray_geometry = ogr.Geometry(ogr.wkbLineString)
                            ray_geometry.AddPoint(point_a_x, point_a_y)
                            ray_geometry.AddPoint(
                                intersection_point.GetX(),
                                intersection_point.GetY())
                            intersection = True
                            break
                    if not intersection:
                        break
                # when we get here `min_point` and `ray_length` are the
                # minimum intersection points for the ray and the landmass
                ray_feature = ogr.Feature(temp_fetch_rays_defn)
                ray_length = ray_geometry.Length()
                ray_feature.SetField('fetch_dist', ray_length)
                ray_feature.SetGeometry(ray_geometry)
                temp_fetch_rays_layer.CreateFeature(ray_feature)
            shore_point_feature.SetField(
                'fdist_%d' % compass_degree, ray_length)
            ray_feature = None
            ray_geometry = None
            rei_value += ray_length * rei_pct * rei_v
        shore_point_feature.SetField('REI', rei_value)
        target_shore_point_layer.SetFeature(shore_point_feature)
        # result_REI[shore_point_fid] = {
        #     'fid': shore_point_fid, 'raw_value': rei_value}
        result_REI[shore_point_fid] = rei_value

    target_shore_point_layer.SyncToDisk()
    target_shore_point_layer = None
    target_shore_point_vector = None
    temp_fetch_rays_layer.SyncToDisk()
    temp_fetch_rays_layer = None
    temp_fetch_rays_vector = None

    file_handle, temp_pickle_path = tempfile.mkstemp(
            dir=os.path.dirname(
                target_wind_exposure_pickle_path), suffix='.pickle')
    os.close(file_handle)
    with open(temp_pickle_path, 'wb') as pickle_file:
        pickle.dump(result_REI, pickle_file)

    _bin_values_to_percentiles(temp_pickle_path, target_wind_exposure_pickle_path)


def calculate_wave_exposure(
        base_fetch_point_vector_path, max_fetch_distance,
        target_wave_exposure_pickle_path):
    """Calculate the wave exposure index.

    Parameters:
        base_fetch_point_vector_path (string): path to a point shapefile that
            contains 16 'WavP_[direction]' fields, 'WavPPCT[direction]'
            fields, 'fdist_[direction]' fields, a single H, and a single T
            field.
        max_fetch_distance (float): max fetch distance before a wind fetch ray
            is terminated
        target_wave_exposure_point_vector_path (string): path to an output
            shapefile that will contain a field called 'Ew' which is the
            maximum of ocean or wind waves occurring at that point.

    Returns:
        None

    """
    LOGGER.info("Calculating wave exposure")
    result_Ew = {}
    base_fetch_point_vector = gdal.OpenEx(
        base_fetch_point_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    base_fetch_point_layer = base_fetch_point_vector.GetLayer()
    for base_fetch_point_feature in base_fetch_point_layer:
        # target_feature = ogr.Feature(target_wave_exposure_point_defn)
        # target_feature.SetGeometry(
        #     base_fetch_point_feature.GetGeometryRef().Clone())
        point_fid = base_fetch_point_feature.GetFID()
        e_local_wedge = (
            0.5 *
            float(base_fetch_point_feature.GetField('H_10PCT'))**2 *
            float(base_fetch_point_feature.GetField('H_10PCT'))) / float(
                _N_FETCH_RAYS)
        e_ocean = 0.0
        e_local = 0.0
        for sample_index in range(_N_FETCH_RAYS):
            compass_degree = int(sample_index * 360 / 16.)
            fdist = base_fetch_point_feature.GetField(
                'fdist_%d' % compass_degree)
            if numpy.isclose(fdist, max_fetch_distance):
                e_ocean += (
                    base_fetch_point_feature.GetField(
                        'WavP_%d' % compass_degree) *
                    base_fetch_point_feature.GetField(
                        'WavPPCT%d' % compass_degree))
            elif fdist > 0.0:
                e_local += e_local_wedge
        # result_Ew[point_fid] = {
        #     'fid': point_fid, 'raw_value': max(e_ocean, e_local)}
        result_Ew[point_fid] = max(e_ocean, e_local)
        # target_feature.SetField('Ew', max(e_ocean, e_local))
        # target_wave_exposure_point_layer.CreateFeature(target_feature)

    file_handle, temp_pickle_path = tempfile.mkstemp(
            dir=os.path.dirname(
                target_wave_exposure_pickle_path), suffix='.pickle')
    os.close(file_handle)
    with open(temp_pickle_path, 'wb') as pickle_file:
        pickle.dump(result_Ew, pickle_file)

    _bin_values_to_percentiles(temp_pickle_path, target_wave_exposure_pickle_path)


def calculate_surge_exposure(
        base_shore_point_vector_path, shelf_contour_path, aoi_vector_path,
        workspace_dir, target_surge_pickle_path):
    """Calculate surge potential as distance to nearest point on a contour.

    Parameters:
        base_shore_point_vector_path (string):  path to a point shapefile to
            for relief point analysis.
        workspace_dir (string): path to a directory to make local calculations
            in
        target_surge_point_vector_path (string): path to output vector.
            after completion will a value for closest distance to continental
            shelf called 'surge'.

    Returns:
        None.

    """
    # gpkg_driver = gdal.GetDriverByName("GPKG")
    # temp_clipped_vector_path = os.path.join(
    #     workspace_dir, 'clipped_shelf_contour.gpkg')
    # for path in [temp_clipped_vector_path]:
    #     if os.path.exists(path):
    #         os.remove(path)
    LOGGER.info("Calculating surge potential")
    aoi_vector_info = pygeoprocessing.get_vector_info(
        aoi_vector_path)
    aoi_ref_wkt = aoi_vector_info['projection']
    aoi_spatial_reference = osr.SpatialReference()
    aoi_spatial_reference.ImportFromWkt(aoi_ref_wkt)
    aoi_bounding_box = aoi_vector_info['bounding_box']

    shelf_ref_wkt = pygeoprocessing.get_vector_info(
        shelf_contour_path)['projection']
    shelf_spatial_reference = osr.SpatialReference()
    shelf_spatial_reference.ImportFromWkt(shelf_ref_wkt)

    shelf_to_aoi_trans = osr.CoordinateTransformation(
        shelf_spatial_reference, aoi_spatial_reference)

    aoi_clipping_box = pygeoprocessing.transform_bounding_box(
        aoi_bounding_box, aoi_ref_wkt,
        shelf_ref_wkt, edge_samples=11)
    aoi_clipping_shapely = shapely.geometry.box(*aoi_clipping_box)

    # this will hold the clipped and transformed shelf geometry
    # temp_clipped_vector = gpkg_driver.Create(
    #     temp_clipped_vector_path, 0, 0, 0, gdal.GDT_Unknown)
    # temp_clipped_layer = (
    #     temp_clipped_vector.CreateLayer(
    #         os.path.basename(os.path.splitext(temp_clipped_vector_path)[0]),
    #         aoi_spatial_reference, ogr.wkbPolygon))
    # temp_clipped_defn = temp_clipped_layer.GetLayerDefn()

    shelf_vector = gdal.OpenEx(
        shelf_contour_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    shelf_layer = shelf_vector.GetLayer()

    # Clip shelf to AOI box, then transform to AOI SRS for distance calc
    clipped_geometry_shapely_list = []
    for shelf_feature in shelf_layer:
        shelf_shapely = shapely.wkb.loads(
            shelf_feature.GetGeometryRef().ExportToWkb())
        intersection_shapely = aoi_clipping_shapely.intersection(
            shelf_shapely)
        if intersection_shapely.is_empty:
            continue
        # if shapely_geometry.is_valid:
        clipped_geometry = ogr.CreateGeometryFromWkt(
            intersection_shapely.wkt)
        err_code = clipped_geometry.Transform(shelf_to_aoi_trans)
        if err_code != 0:
            raise ValueError(
                "Could not project %s to spatial reference system of %s"
                % (shelf_contour_path, aoi_vector_path))
        clipped_geometry_shapely_list.append(shapely.wkb.loads(
            clipped_geometry.ExportToWkb()))
        # clipped_feature = ogr.Feature(temp_clipped_defn)
        # clipped_feature.SetGeometry(clipped_geometry)
        # temp_clipped_layer.CreateFeature(clipped_feature)
        # clipped_feature = None
    shelf_vector = None
    shelf_layer = None

    shelf_shapely_union = shapely.ops.cascaded_union(
        clipped_geometry_shapely_list)

    if not clipped_geometry_shapely_list:
        # TODO: would be nice to validate this early in runtime,
        # or maybe issue warning here and calculate exposure w/o surge potental?
        raise ValueError(
            "The AOI polygon (%s) and the shelf contour line (%s)"
            "do not intersect" % (aoi_vector_path, shelf_contour_path))

    shore_point_vector = gdal.OpenEx(
        base_shore_point_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    shore_point_layer = shore_point_vector.GetLayer()
    result = {}
    for point_feature in shore_point_layer:
        point_fid = point_feature.GetFID()
        point_geometry = point_feature.GetGeometryRef()
        point_shapely = shapely.wkb.loads(point_geometry.ExportToWkb())
        result[point_fid] = point_shapely.distance(shelf_shapely_union)

    file_handle, temp_pickle_path = tempfile.mkstemp(
        dir=workspace_dir, suffix='.pickle')
    os.close(file_handle)
    with open(temp_pickle_path, 'wb') as pickle_file:
        pickle.dump(result, pickle_file)
    with open(os.path.join(workspace_dir, 'temp_surge.json'), 'w') as json_file:
        json.dump(result, json_file)
    _bin_values_to_percentiles(temp_pickle_path, target_surge_pickle_path)


# def calculate_surge_exposure_bathy(
#         base_shore_point_vector_path, bathymetry_dem_path, aoi_vector_path,
#         smallest_feature_size, depth_contour, workspace_dir,
#         target_surge_pickle_path):
#     """Calculate surge potential as distance to a bathymetry depth contour.

#     Parameters:
#         base_shore_point_vector_path (string):  path to a point shapefile to
#             for relief point analysis.
#         global_dem_path (string): path to a DEM raster projected in wgs84.
#         workspace_dir (string): path to a directory to make local calculations
#             in
#         target_surge_point_vector_path (string): path to output vector.
#             after completion will a value for closest distance to continental
#             shelf called 'surge'.

#     Returns:
#         None.

#     """
#     clipped_dem_path = os.path.join(
#         workspace_dir, 'clipped_dem.tif')
#     clipped_utm_dem_path = os.path.join(
#         workspace_dir, 'clipped_utm_dem.tif')
#     shelf_mask_path = os.path.join(
#         workspace_dir, 'shelf_mask.tif')
#     shelf_kernel_path = os.path.join(workspace_dir, 'shelf_kernel.tif')
#     shelf_convoultion_raster_path = os.path.join(
#         workspace_dir, 'shelf_convolution.tif')
#     masked_shelf_convolution_raster_path = os.path.join(
#         workspace_dir, 'masked_shelf_convolution.tif')
#     shelf_edge_raster_path = os.path.join(workspace_dir, 'shelf_edge.tif')

#     if depth_contour >= 0:
#         err_msg = "depth contour and bathymetry values should be negative"
#         # TODO: is this redundant, as in two log messages result?
#         LOGGER.error(err_msg)
#         raise ValueError(err_msg)
#     aoi_vector_info = pygeoprocessing.get_vector_info(
#         aoi_vector_path)
#     base_ref_wkt = aoi_vector_info['projection']
#     base_spatial_reference = osr.SpatialReference()
#     base_spatial_reference.ImportFromWkt(base_ref_wkt)

#     aoi_bounding_box = aoi_vector_info['bounding_box']
#     pixel_buffer = 1
#     aoi_bounding_box[0] -= pixel_buffer * smallest_feature_size
#     aoi_bounding_box[1] -= pixel_buffer * smallest_feature_size
#     aoi_bounding_box[2] += pixel_buffer * smallest_feature_size
#     aoi_bounding_box[3] += pixel_buffer * smallest_feature_size

#     dem_ref_wkt = pygeoprocessing.get_raster_info(bathymetry_dem_path)['projection']

#     aoi_clipping_box = pygeoprocessing.transform_bounding_box(
#         aoi_bounding_box, base_ref_wkt,
#         dem_ref_wkt, edge_samples=11)

#     target_pixel_size = pygeoprocessing.get_raster_info(
#         bathymetry_dem_path)['pixel_size']
#     # TODO: this warp could also be a mask_raster, maybe?
#     # TODO: is it really best to do 2 warps? 1st just clips to bbox. 2nd does transform.
#     pygeoprocessing.warp_raster(
#         bathymetry_dem_path, target_pixel_size, clipped_dem_path,
#         'bilinear', target_bb=aoi_clipping_box)
#     # TODO: best to warp into pixels of user-defined size or into
#     # pixels of size close to original size? user-defined size could
#     # result in big downscaling/upscaling
#     target_pixel_size = (
#         smallest_feature_size / 2.0, -smallest_feature_size / 2.0)
#     pygeoprocessing.warp_raster(
#         clipped_dem_path, target_pixel_size, clipped_utm_dem_path,
#         'bilinear', target_sr_wkt=base_ref_wkt)
#     nodata = pygeoprocessing.get_raster_info(
#         clipped_utm_dem_path)['nodata'][0]
#     shelf_nodata = 2

#     def mask_shelf(depth_array, depth_contour):
#         result_array = numpy.empty(
#             depth_array.shape, dtype=numpy.int16)
#         if nodata is not None:
#             valid_mask = depth_array != nodata
#             result_array[:] = shelf_nodata
#             result_array[valid_mask] = 0
#         else:
#             result_array[:] = 0
#         result_array[depth_array < depth_contour] = 1
#         return result_array

#     pygeoprocessing.raster_calculator(
#         [(clipped_utm_dem_path, 1), (depth_contour, 'raw')], mask_shelf,
#         shelf_mask_path, gdal.GDT_Byte, shelf_nodata)

#     # convolve to find edges
#     # grid shoreline from raster
#     make_shore_kernel(shelf_kernel_path)
#     pygeoprocessing.convolve_2d(
#         (shelf_mask_path, 1), (shelf_kernel_path, 1),
#         shelf_convoultion_raster_path, target_datatype=gdal.GDT_Byte,
#         target_nodata=255)
#     # Apply mask to trim off edges of the convolution, which
#     # incorrectly categorize as shelf edge.
#     pygeoprocessing.mask_raster(
#         (shelf_convoultion_raster_path, 1), aoi_vector_path,
#         masked_shelf_convolution_raster_path, all_touched=False)

#     nodata = pygeoprocessing.get_raster_info(
#         shelf_convoultion_raster_path)['nodata'][0]

#     def _shelf_mask_op(shelf_convolution):
#         """Mask values on land that border the continental shelf."""
#         result = numpy.empty(shelf_convolution.shape, dtype=numpy.uint8)
#         result[:] = nodata
#         valid_mask = shelf_convolution != nodata
#         # If a pixel is on land, it gets at least a 9, but if it's all on
#         # land it gets an 17 (8 neighboring pixels), so we search between 9
#         # and 17 to determine a shore pixel
#         result[valid_mask] = numpy.where(
#             (shelf_convolution[valid_mask] >= 9) &
#             (shelf_convolution[valid_mask] < 17), 1, nodata)
#         return result

#     pygeoprocessing.raster_calculator(
#         [(masked_shelf_convolution_raster_path, 1)], _shelf_mask_op,
#         shelf_edge_raster_path, gdal.GDT_Byte, nodata)

#     shore_geotransform = pygeoprocessing.get_raster_info(
#         shelf_edge_raster_path)['geotransform']

#     shelf_rtree = rtree.index.Index()

#     # Get coordinates of all pixels on the contour
#     for offset_info, data_block in pygeoprocessing.iterblocks(
#             (shelf_edge_raster_path, 1)):
#         row_indexes, col_indexes = numpy.mgrid[
#             offset_info['yoff']:offset_info['yoff']+offset_info['win_ysize'],
#             offset_info['xoff']:offset_info['xoff']+offset_info['win_xsize']]
#         valid_mask = data_block == 1
#         x_coordinates = (
#             shore_geotransform[0] +
#             shore_geotransform[1] * (col_indexes[valid_mask] + 0.5) +
#             shore_geotransform[2] * (row_indexes[valid_mask] + 0.5))
#         y_coordinates = (
#             shore_geotransform[3] +
#             shore_geotransform[4] * (col_indexes[valid_mask] + 0.5) +
#             shore_geotransform[5] * (row_indexes[valid_mask] + 0.5))

#         for x_coord, y_coord in zip(x_coordinates, y_coordinates):
#             shelf_rtree.insert(
#                 0, [x_coord, y_coord, x_coord, y_coord],
#                 obj=shapely.geometry.Point(x_coord, y_coord))

#     shore_point_vector = gdal.OpenEx(
#         base_shore_point_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
#     shore_point_layer = shore_point_vector.GetLayer()

#     result = {}
#     for point_feature in shore_point_layer:
#         point_fid = point_feature.GetFID()
#         point_geometry = point_feature.GetGeometryRef()
#         point_shapely = shapely.wkb.loads(point_geometry.ExportToWkb())
#         nearest_point = list(shelf_rtree.nearest(
#                 (point_geometry.GetX(),
#                  point_geometry.GetY(),
#                  point_geometry.GetX(),
#                  point_geometry.GetY()),
#                 objects='raw', num_results=1))
#         if len(nearest_point) > 0:
#             distance = nearest_point[0].distance(point_shapely)
#             result[point_fid] = float(distance)
#             # point_feature.SetField('surge', float(distance))
#         else:
#             # If we're here for one point, we're here for all points because
#             # the AOI extent did not include any pixels below the depth contour
#             # TODO: raise an error or return a uniform value?
#             result[point_fid] = 0.0  # value doesn't matter since all points get same
#             # point_feature.SetField('surge', 0.0)

#     file_handle, temp_pickle_path = tempfile.mkstemp(
#         dir=workspace_dir, suffix='.pickle')
#     os.close(file_handle)
#     with open(temp_pickle_path, 'wb') as pickle_file:
#         pickle.dump(result, pickle_file)
#     _bin_values_to_percentiles(temp_pickle_path, target_surge_pickle_path)


def calculate_relief_exposure(
        base_shore_point_vector_path, global_dem_path, dem_averaging_radius,
        smallest_feature_size, workspace_dir, target_relief_pickle_path):
    """Calculate DEM relief as average coastal land area within 5km.

    Parameters:
        base_shore_point_vector_path (string):  path to a point shapefile to
            for relief point analysis.
        global_dem_path (string): path to a DEM raster projected in wgs84.
        workspace_dir (string): path to a directory to make local calculations
            in
        target_relief_point_vector_path (string): path to output vector.
            after completion will a value for average relief within 5km in
            a field called 'relief'.

    Returns:
        None.

    """
    LOGGER.info("Calculating relief exposure")
    # SRS here is inherited from the user's AOI
    base_shore_point_info = pygeoprocessing.get_vector_info(
        base_shore_point_vector_path)
    base_ref_wkt = base_shore_point_info['projection']
    base_spatial_reference = osr.SpatialReference()
    base_spatial_reference.ImportFromWkt(base_ref_wkt)

    shore_point_bounding_box = base_shore_point_info['bounding_box']
    shore_point_bounding_box[0] -= dem_averaging_radius
    shore_point_bounding_box[1] -= dem_averaging_radius
    shore_point_bounding_box[2] += dem_averaging_radius
    shore_point_bounding_box[3] += dem_averaging_radius

    dem_ref_wkt = pygeoprocessing.get_raster_info(global_dem_path)['projection']

    shore_point_clipping_box = pygeoprocessing.transform_bounding_box(
        shore_point_bounding_box, base_ref_wkt,
        dem_ref_wkt, edge_samples=11)

    clipped_dem_path = os.path.join(
        workspace_dir, 'clipped_dem.tif')

    target_pixel_size = pygeoprocessing.get_raster_info(
        global_dem_path)['pixel_size']
    # TODO: this warp could also be a mask_raster, maybe
    # TODO: is it really best to do 2 warps? 1st just clips to bbox. 2nd does transform.
    pygeoprocessing.warp_raster(
        global_dem_path, target_pixel_size, clipped_dem_path,
        'bilinear', target_bb=shore_point_clipping_box)
    clipped_utm_dem_path = os.path.join(
        workspace_dir, 'clipped_utm_dem.tif')
    target_pixel_size = (
        smallest_feature_size / 2.0, -smallest_feature_size / 2.0)
    pygeoprocessing.warp_raster(
        clipped_dem_path, target_pixel_size, clipped_utm_dem_path,
        'bilinear', target_sr_wkt=base_ref_wkt)
    # mask out all DEM < 0 to 0
    nodata = pygeoprocessing.get_raster_info(
        clipped_utm_dem_path)['nodata'][0]

    def zero_negative_values(depth_array):
        valid_mask = depth_array != nodata
        result_array = numpy.empty(
            depth_array.shape, dtype=numpy.int16)
        result_array[:] = nodata
        result_array[valid_mask] = 0
        result_array[depth_array > 0] = depth_array[depth_array > 0]
        return result_array

    positive_dem_path = os.path.join(
        workspace_dir, 'positive_dem.tif')

    pygeoprocessing.raster_calculator(
        [(clipped_utm_dem_path, 1)], zero_negative_values,
        positive_dem_path, gdal.GDT_Int16, nodata)

    # convolve over a user-defined radius
    radius_in_pixels = dem_averaging_radius / target_pixel_size[0]
    kernel_filepath = os.path.join(workspace_dir, 'averaging_kernel.tif')
    create_averaging_kernel_raster(radius_in_pixels, kernel_filepath)

    relief_path = os.path.join(workspace_dir, 'relief.tif')
    pygeoprocessing.convolve_2d(
        (positive_dem_path, 1), (kernel_filepath, 1),
        relief_path, ignore_nodata=True)

    file_handle, temp_pickle_path = tempfile.mkstemp(
        dir=workspace_dir, suffix='.pickle')
    os.close(file_handle)
    _extract_raster_values_by_points(
        base_shore_point_vector_path, relief_path,
        temp_pickle_path)

    _bin_values_to_percentiles(temp_pickle_path, target_relief_pickle_path)


def _bin_values_to_percentiles(
        base_values_pickle_path, target_values_pickle_path):

    # TODO: too much decosntructing and reconstructing of these dicts?
    with open(base_values_pickle_path, 'rb') as pickle_file:
        base_values_dict = pickle.load(pickle_file)
    base_values = base_values_dict.values()
    # Relief is unique because high elevation values mean low exposure
    if os.path.basename(target_values_pickle_path) == 'relief.pickle':
        base_values = numpy.multiply(base_values, -1.0)
    target_risk_array = numpy.searchsorted(
                numpy.percentile(
                    base_values, [20, 40, 60, 80, 100]),
                base_values) + 1
    fids = base_values_dict.keys()
    target_values_dict = {}
    for fid, rank in zip(fids, target_risk_array):
        target_values_dict[fid] = rank
    with open(target_values_pickle_path, 'wb') as pickle_file:
        pickle.dump(target_values_dict, pickle_file)


def _extract_raster_values_by_points(
        base_point_vector_path, base_raster_path, target_pickle_path):

    vector = gdal.OpenEx(
        base_point_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    layer = vector.GetLayer()

    raster = gdal.Open(base_raster_path)
    band = raster.GetRasterBand(1)
    n_rows = band.YSize
    result = {}
    raster_geotransform = raster.GetGeoTransform()
    for point_feature in layer:
        point_geometry = point_feature.GetGeometryRef()
        point_x, point_y = point_geometry.GetX(), point_geometry.GetY()
        point_fid = point_feature.GetFID()
        point_geometry = None
        pixel_x = int(
            (point_x - raster_geotransform[0]) / raster_geotransform[1])
        pixel_y = int(
            (point_y - raster_geotransform[3]) / raster_geotransform[5])

        if pixel_y >= n_rows:
            pixel_y = n_rows - 1
        try:
            pixel_value = band.ReadAsArray(
                xoff=pixel_x, yoff=pixel_y, win_xsize=1,
                win_ysize=1)[0, 0]
        except Exception:
            LOGGER.error(
                'band size %d %d', band.XSize, band.YSize)
            raise
        # result[point_fid] = {'fid': point_fid, value_keyname: pixel_value}
        result[point_fid] = pixel_value
    with open(target_pickle_path, 'wb') as pickle_file:
        pickle.dump(result, pickle_file)


def calculate_habitat_protection(
        base_shore_point_vector_path,
        habitat_table_path, workspace_dir,
        target_habitat_protection_path):
    """Calculate habitat protection at a set of points.

    Mandate that habitat layers use the same projected SRS as AOI. We won't 
    reproject anything here. 

    Parameters:
        base_shore_point_vector_path (string):  path to a point shapefile to
            analyze habitat protection at.
        habitat_layer_lookup: a dictionary mapping habitat id to a
            (path, rank, distance) tuple
        workspace_dir (string): path to a directory to make local calculations
            in
        target_habitat_protection_point_vector_path (string): path to desired
            output vector.  after completion will have a rank for each
            habitat ID, and a field called Rhab with a value from 1-5
            indicating relative level of protection of that point.

    Returns:
        None.

    """
    LOGGER.info("Calculating habitat protection")
    habitat_dataframe = (pandas.read_csv(
        habitat_table_path, header=0,
        dtype={'habitat': str, 'path': str, 'rank': numpy.int16,
               'protection distance (m)': numpy.int16})
        .rename(columns={'protection distance (m)': 'distance'}))

    # SRS here is inherited from the user's AOI
    base_shore_point_info = pygeoprocessing.get_vector_info(
        base_shore_point_vector_path)
    base_ref_wkt = base_shore_point_info['projection']
    base_spatial_reference = osr.SpatialReference()
    base_spatial_reference.ImportFromWkt(base_ref_wkt)

    max_habitat_search_distance = habitat_dataframe['distance'].max()
    shore_point_bounding_box = base_shore_point_info['bounding_box']
    shore_point_bounding_box[0] -= max_habitat_search_distance
    shore_point_bounding_box[1] -= max_habitat_search_distance
    shore_point_bounding_box[2] += max_habitat_search_distance
    shore_point_bounding_box[3] += max_habitat_search_distance
    shore_point_clipping_shapely = shapely.geometry.box(*shore_point_bounding_box)

    habitat_shapely_lookup = {}
    for habitat_row in habitat_dataframe.itertuples():
        habitat_vector = gdal.OpenEx(
            _sanitize_path(habitat_table_path, habitat_row.path),
            gdal.OF_VECTOR | gdal.GA_ReadOnly)
        habitat_layer = habitat_vector.GetLayer()

        # this will hold the clipped habitat geometry
        gpkg_driver = gdal.GetDriverByName("GPKG")
        temp_clipped_vector_path = os.path.join(
            workspace_dir, 'clipped_habitat_%s.gpkg' % habitat_row.id)
        utm_clipped_vector_path = os.path.join(
            workspace_dir, 'utm_clipped_habitat_%s.gpkg' % habitat_row.id)
        for path in [temp_clipped_vector_path, utm_clipped_vector_path]:
            if os.path.exists(path):
                os.remove(path)
        temp_clipped_vector = gpkg_driver.Create(
            temp_clipped_vector_path, 0, 0, 0, gdal.GDT_Unknown)
        temp_clipped_layer = (
            temp_clipped_vector.CreateLayer(
                os.path.basename(os.path.splitext(temp_clipped_vector_path)[0]),
                base_spatial_reference, ogr.wkbPolygon))
        temp_clipped_defn = temp_clipped_layer.GetLayerDefn()

        # clip global polygon to global clipping box
        clipped_geometry_shapely_list = []
        for habitat_feature in habitat_layer:
            habitat_shapely = shapely.wkb.loads(
                habitat_feature.GetGeometryRef().ExportToWkb())
            intersection_shapely = shore_point_clipping_shapely.intersection(
                habitat_shapely)
            if intersection_shapely.is_empty:
                continue
            try:
                # if shapely_geometry.is_valid:
                clipped_geometry_shapely_list.append(intersection_shapely)
                clipped_geometry = ogr.CreateGeometryFromWkt(
                    intersection_shapely.wkt)
                clipped_feature = ogr.Feature(temp_clipped_defn)
                clipped_feature.SetGeometry(clipped_geometry)
                temp_clipped_layer.CreateFeature(clipped_feature)
                clipped_feature = None
            except Exception:
                LOGGER.warn(
                    "Couldn't process this intersection %s",
                    intersection_shapely)

        habitat_shapely_lookup[habitat_row.id] = shapely.ops.cascaded_union(
            clipped_geometry_shapely_list)
        temp_clipped_layer.SyncToDisk()
        temp_clipped_layer = None
        temp_clipped_vector = None
        habitat_vector = None
        habitat_layer = None

    base_shore_point_vector = gdal.OpenEx(
        base_shore_point_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    base_shore_point_layer = base_shore_point_vector.GetLayer()

    point_habitat_rank_list = []  # {0: {'fid': 0, 'kelp': 4, 'eelgrass': 3}}
    for target_feature in base_shore_point_layer:
        point_habitat_rank_dict = {}
        target_feature_geometry = (
            target_feature.GetGeometryRef().Clone())
        target_feature_shapely = shapely.wkb.loads(
            target_feature_geometry.ExportToWkb())
        fid = target_feature.GetFID()
        point_habitat_rank_dict['fid'] = fid

        for habitat_row in habitat_dataframe.itertuples():
            point_habitat_rank = 5  # represents no habitat protection
            if habitat_shapely_lookup[habitat_row.id].is_empty:
                # TODO: how to actually handle this case of no geom in a habitat layer?
                # it's quite unlikely...
                continue
            point_distance_to_feature = target_feature_shapely.distance(
                habitat_shapely_lookup[habitat_row.id])
            if point_distance_to_feature <= habitat_row.distance:
                point_habitat_rank = habitat_row.rank
            point_habitat_rank_dict[habitat_row.id] = point_habitat_rank
        point_habitat_rank_list.append(point_habitat_rank_dict)

    habitat_ranks_dataframe = pandas.DataFrame(point_habitat_rank_list)

    def _calc_Rhab(row):
        # Equation 4
        sum_sq_rank = 0.0
        min_rank = 5
        for r in row:
            if r < min_rank:
                min_rank = r
            sum_sq_rank += (5 - r)**2

        if sum_sq_rank > 0:
            r_hab_val = max(
                1.0, 4.8 - 0.5 * (
                    (1.5 * (5-min_rank))**2 + sum_sq_rank -
                    (5-min_rank)**2)**0.5)
        else:
            r_hab_val = 5.0
        return r_hab_val

    habitat_ranks_dataframe['Rhab'] = (
        habitat_ranks_dataframe.drop(columns='fid')
        .apply(axis=1, func=_calc_Rhab))

    habitat_ranks_dataframe.to_csv(target_habitat_protection_path, index=False)


def calculate_geomorphology_exposure(
        geomorphology_vector_path, grid_raster_path, working_dir,
        target_geomorphology_raster_path, base_shore_point_vector_path,
        target_pickle_path):
    
    # TODO: right now the call to gdal_rasterize ends of reprojecting
    # the input vector to the target raster SRS. maybe do this explicity first instead.
    LOGGER.info("Assigning geomorphology rank")
    vector = gdal.OpenEx(geomorphology_vector_path)
    # TODO: what happens if we select for a rank not present in shp?
    acceptable_ranks = (1, 2, 3, 4, 5)
    nodata_value = 9 
    fill_value = 3 
    template_raster = gdal.OpenEx(grid_raster_path, gdal.OF_RASTER)
    driver = gdal.GetDriverByName('GTiff')
    driver.CreateCopy(
        target_geomorphology_raster_path, template_raster)
    template_raster = None
    target_geomorphology_raster = gdal.OpenEx(
        target_geomorphology_raster_path, gdal.OF_RASTER | gdal.GA_Update)
    band = target_geomorphology_raster.GetRasterBand(1)
    band.Fill(nodata_value)
    band.SetNoDataValue(nodata_value)
    band.FlushCache()
    band = None
    target_geomorphology_raster = None

    geomorphology_raster_path_band_list = []
    for rank in acceptable_ranks:
        layer = vector.GetLayer()
        filter_string = ('RANK = %d' % rank)
        layer.SetAttributeFilter(str(filter_string))
        target_raster_path = os.path.join(
            working_dir, 'geomorphology_rank_%d.tif' % rank)
        geomorphology_raster_path_band_list.append((target_raster_path, 1))
        _rasterize_vector_onto_base(
            target_geomorphology_raster_path, geomorphology_vector_path,
            'RANK', target_raster_path, filter_string=filter_string)
        layer = None

    # TODO: for points with no geomorph data, we fill with a default
    # Is this okay? or should we fill with neighboring values/average/mode etc?
    def _min_op(nodata, fill_value, *rgeo):
        result = numpy.amin(rgeo, axis=0)
        result[result == nodata] = fill_value
        return result

    pygeoprocessing.raster_calculator(
        [(nodata_value, 'raw'), (fill_value, 'raw')] +
        geomorphology_raster_path_band_list,
        _min_op, target_geomorphology_raster_path,
        gdal.GDT_Byte, nodata_value)

    _extract_raster_values_by_points(
        base_shore_point_vector_path, target_geomorphology_raster_path,
        target_pickle_path)


def aggregate_raster_data(
        raster_feature_id_map,
        base_point_vector_path,
        target_result_point_vector_path):
    """Summarize population data around each shore point.

    Parameters:
        raster_feature_id_map (dict): maps feature id names to a 3-tuple:
            * path to a raster to sample for each point in
            `base_point_vector_path
            * boolean indicating whether that value should be divided by the
              pixel area.
            * if not None, a list of raster ids that should be masked to 1
              and everything else to 0. If so result is proportion of 1s in
              sampled area within `sample_distance`
        base_point_vector_path (path): a global point vector path
            that is to be used for the base of the result
        sample_distance (float): distance in meters to sample raster values
            around the point.
        target_result_point_vector_path (path): will contain all the points
            in base_point_vector_path with additional fields mapping
            to the `raster_feature_id_map` keys.

    Returns:
        None.

    """
    if os.path.exists(target_result_point_vector_path):
        os.remove(target_result_point_vector_path)
    base_vector = gdal.OpenEx(base_point_vector_path, gdal.OF_VECTOR)
    mem_result_point_vector = gdal.GetDriverByName('MEMORY').CreateCopy(
        '', base_vector)

    mem_result_point_layer = mem_result_point_vector.GetLayer()
    for simulation_id in raster_feature_id_map:
        mem_result_point_layer.CreateField(
            ogr.FieldDefn(simulation_id, ogr.OFTReal))

    # this is for sea level rise
    mem_result_point_layer.CreateField(
        ogr.FieldDefn('SLRrise_cur', ogr.OFTReal))
    mem_result_point_layer.CreateField(
        ogr.FieldDefn('Rslr_cur', ogr.OFTReal))
    for ssp_id, rcp_id in [(1, 26), (3, 60), (5, 85)]:
        mem_result_point_layer.CreateField(
            ogr.FieldDefn('SLRrise_ssp%d' % ssp_id, ogr.OFTReal))
        mem_result_point_layer.CreateField(
            ogr.FieldDefn('Rhab_ssp%d' % ssp_id, ogr.OFTReal))
        mem_result_point_layer.CreateField(
            ogr.FieldDefn('curpb_ssp%d' % ssp_id, ogr.OFTReal))
        mem_result_point_layer.CreateField(
            ogr.FieldDefn('cpdn_ssp%d' % ssp_id, ogr.OFTReal))
        mem_result_point_layer.CreateField(
            ogr.FieldDefn('Rslr_ssp%d' % ssp_id, ogr.OFTReal))

    # recalibrated population fields for each ssp
    for ssp_id in (1, 3, 5):
        mem_result_point_layer.CreateField(
            ogr.FieldDefn('pdnrc_ssp%d' % ssp_id, ogr.OFTReal))

    for scenario_id in ['cur', 'ssp1', 'ssp3', 'ssp5']:
        mem_result_point_layer.CreateField(
            ogr.FieldDefn('Service_%s' % scenario_id, ogr.OFTReal))
        mem_result_point_layer.CreateField(
            ogr.FieldDefn('NCP_%s' % scenario_id, ogr.OFTReal))

    #UPDATE cv_table SET pdnrc_ssp1 = pdn_gpw * cpdn_ssp1;
    #UPDATE cv_table SET Service_cur = Rtnohab_cur - Rt_cur;
    #UPDATE cv_table SET NCP_cur = Service_cur / Rtnohab_cur;

    for simulation_id, (
                raster_path, divide_by_area, reclass_ids, sample_distance,
                extra_pixel) in (
            raster_feature_id_map.items()):
        raster = gdal.Open(raster_path)
        LOGGER.debug("processing aggregation %s", simulation_id)
        if not raster:
            LOGGER.debug("processing for aggregation %s failed", raster_path)
        band = raster.GetRasterBand(1)
        n_rows = band.YSize
        n_cols = band.XSize
        geotransform = raster.GetGeoTransform()
        nodata = band.GetNoDataValue()

        mem_result_point_layer.ResetReading()
        mem_result_point_layer.StartTransaction()
        for point_feature in mem_result_point_layer:
            point_geometry = point_feature.GetGeometryRef()
            point_x = point_geometry.GetX()
            point_y = point_geometry.GetY()

            lng_m, lat_m = lat_to_meters(point_y)
            pixel_dist_x = int(abs(
                sample_distance / (lng_m * geotransform[1]))) + extra_pixel
            pixel_dist_y = int(abs(
                sample_distance / (lat_m * geotransform[5]))) + extra_pixel
            point_geometry = None

            # this handles the case where slr goes to 360
            if geotransform[0] == 0 and point_x < 0:
                point_x += 360

            pixel_x = int(
                (point_x - geotransform[0]) /
                geotransform[1]) - pixel_dist_x
            pixel_y = int(
                (point_y - geotransform[3]) /
                geotransform[5]) - pixel_dist_y

            if pixel_x < 0:
                pixel_x = 0
            if pixel_y < 0:
                pixel_y = 0
            win_xsize = 1 + pixel_dist_x + extra_pixel
            win_ysize = 1 + pixel_dist_y + extra_pixel
            if pixel_x + win_xsize >= n_cols:
                win_xsize = n_cols - pixel_x - 1
            if pixel_y + win_ysize >= n_rows:
                win_ysize = n_rows - pixel_y - 1
            if win_xsize <= 0 or win_ysize <= 0:
                pixel_value = 0
            else:
                try:
                    array = band.ReadAsArray(
                        xoff=pixel_x, yoff=pixel_y, win_xsize=win_xsize,
                        win_ysize=win_ysize)
                    if nodata is not None and reclass_ids is None:
                        mask = array != nodata
                    else:
                        mask = numpy.ones(array.shape, dtype=numpy.bool)
                    if reclass_ids is not None:
                        pixel_value = numpy.mean(
                            numpy.in1d(array.flatten(), reclass_ids))
                    elif numpy.count_nonzero(mask) > 0:
                        pixel_value = numpy.max(array[mask])
                    else:
                        pixel_value = 0
                except Exception:
                    LOGGER.error(
                        'band size %d %d', band.XSize,
                        band.YSize)
                    raise
            # calculate pixel area in sq km
            if divide_by_area:
                pixel_area_km = abs(
                    (lng_m * geotransform[1]) *
                    (lat_m * geotransform[5])) / 1e6
                pixel_value /= pixel_area_km
            point_feature.SetField(simulation_id, float(pixel_value))
            mem_result_point_layer.SetFeature(point_feature)
        mem_result_point_layer.CommitTransaction()

    mem_result_point_layer.ResetReading()
    mem_result_point_layer.StartTransaction()
    # use this list to calculate risks

    # this list is used to hold all the sea level rise values for cur, ssp1,
    # 3, and 5, hence the length of *4 for the list. it so we can calculate
    # a constant rank for all of them.
    n_points = mem_result_point_layer.GetFeatureCount()
    slr_rise_value_list = numpy.empty(n_points*4)
    fid_index_map = {}
    LOGGER.debug('gathering all the sea level rise data')
    for point_index, point_feature in enumerate(mem_result_point_layer):
        slrrise_val = point_feature.GetField('SLRrate_cur') * 25. / 1000.
        point_feature.SetField('SLRrise_cur', slrrise_val)
        slr_rise_value_list[point_index] = slrrise_val
        fid_index_map[point_index] = point_feature.GetFID()
        for ssp_offset, (ssp_id, rcp_id) in enumerate(
                [(1, 26), (3, 60), (5, 85)]):
            slrrise_val = point_feature.GetField('slr_rcp%d' % rcp_id)
            # this offsets the index by n_points * whatever ssp it is in order
            slr_rise_value_list[point_index + (ssp_offset+1)*n_points] = (
                slrrise_val)
            point_feature.SetField('SLRrise_ssp%d' % ssp_id, slrrise_val)

            point_feature.SetField(
                'Rhab_ssp%d' % ssp_id, (
                    5. - point_feature.GetField('Rhab_cur')) * (
                        point_feature.GetField('urbp_ssp%d' % ssp_id) -
                        point_feature.GetField('urbp_2015')) +
                point_feature.GetField('Rhab_cur'))

            point_feature.SetField(
                'curpb_ssp%d' % ssp_id, (
                    point_feature.GetField('urbp_ssp%d' % ssp_id) -
                    point_feature.GetField('urbp_2015')))

            if point_feature.GetField('pdn_2010') != 0:
                point_feature.SetField(
                    'cpdn_ssp%d' % ssp_id, (
                        point_feature.GetField('pdn_ssp%d' % ssp_id) /
                        point_feature.GetField('pdn_2010')))
            else:
                point_feature.SetField('cpdn_ssp%d' % ssp_id, 0.0)

        #Rt_ssp[1|3|5] = recalculated using Rhab_ssp[1|3|5] and Rslr_ssp[1|3|5]
        #Rt_cur_nh = Rt_cur calculated with Rhab = 5
        #Rt_ssp[1|3|5]_nh = Rt_ssp[1|3|5] calculated with Rhab = 5
        mem_result_point_layer.SetFeature(point_feature)
    LOGGER.debug('calculating Rslr')
    slr_risk_array = numpy.searchsorted(
        numpy.percentile(slr_rise_value_list, [20, 40, 60, 80, 100]),
        slr_rise_value_list) + 1
    for point_index in range(n_points):
        point_feature = mem_result_point_layer.GetFeature(
            fid_index_map[point_index])
        point_feature.SetField('Rslr_cur', float(slr_risk_array[point_index]))
        point_feature.SetField(
            'Rslr_ssp1', float(slr_risk_array[point_index+n_points]))
        point_feature.SetField(
            'Rslr_ssp3', float(slr_risk_array[point_index+n_points*2]))
        point_feature.SetField(
            'Rslr_ssp5', float(slr_risk_array[point_index+n_points*3]))
        mem_result_point_layer.SetFeature(point_feature)
    mem_result_point_layer.CommitTransaction()
    mem_result_point_layer.SyncToDisk()
    mem_result_point_layer = None
    gdal.GetDriverByName('GPKG').CreateCopy(
        target_result_point_vector_path, mem_result_point_vector)


def calculate_final_risk(
        risk_id_path_list, habitat_protection_path,
        base_point_vector_path,
        target_point_vector_path):
    """Calculate the final Rt and Rtnohabs for cur, ssp1, 3, and 5.

    Parameters:
        risk_id_list (list): a list of tuples of the form
        (Rt_[cur|ssp{1,3,5}], (list of risk feature ids or constants))
        target_point_vector_path (string): path to vector to modify. Will contain
            at least the fields defined in

            Rt_cur, Rt_ssp1, Rt_ssp3, Rt_ssp5
            Rtnohab_cur, Rtnohab_ssp1, Rtnohab_ssp3, Rtnohab_ssp5

    Returns:
        None.

    """
    LOGGER.info("Calculating final coastal exposure and habitat role")
    driver = gdal.GetDriverByName('GPKG')
    if os.path.exists(target_point_vector_path):
        driver.Delete(target_point_vector_path)
    base_point_vector = gdal.OpenEx(
        base_point_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    target_point_vector = driver.CreateCopy(
        target_point_vector_path, base_point_vector)
    base_point_vector = None

    layer = target_point_vector.GetLayer()

    risk_id_list_no_hab = []
    # All intermediate risk ranks except habitat are in pickles
    for risk_filename in risk_id_path_list:
        risk_id = os.path.basename(os.path.splitext(risk_filename)[0])
        risk_id_list_no_hab.append(risk_id)
        # Create a new field for the variable
        # TODO: are they actually all ints?
        risk_field = ogr.FieldDefn(str(risk_id), ogr.OFTReal)
        risk_field.SetWidth(24)
        risk_field.SetPrecision(11)
        layer.CreateField(risk_field)
        with open(risk_filename, 'rb') as file:
            risk_dict = pickle.load(file)
        for feature_id, rank in risk_dict.iteritems():
            feature = layer.GetFeature(int(feature_id))
            feature.SetField(str(risk_id), float(rank))
            layer.SetFeature(feature)

    # Intermediate habitat ranks are in a CSV
    habitat_dataframe = pandas.read_csv(habitat_protection_path)
    risk_id = 'Rhab'
    risk_id_list = risk_id_list_no_hab[:]
    risk_id_list.append(risk_id)
    risk_field = ogr.FieldDefn(str(risk_id), ogr.OFTReal)
    risk_field.SetWidth(24)
    risk_field.SetPrecision(11)
    layer.CreateField(risk_field)

    for feature_id, rank in zip(
            habitat_dataframe['fid'], habitat_dataframe[risk_id]):
        feature = layer.GetFeature(int(feature_id))
        feature.SetField(str(risk_id), float(rank))
        layer.SetFeature(feature)

    layer = None
    target_point_vector.FlushCache()
    target_point_vector = None

    target_point_vector = gdal.OpenEx(
        target_point_vector_path, gdal.OF_VECTOR | gdal.GA_Update)
    target_result_point_layer = target_point_vector.GetLayer()
    target_result_point_layer.ResetReading()
    final_risk = 'risk'
    habitat_role = 'hab_role'
    target_result_point_layer.CreateField(
        ogr.FieldDefn(final_risk, ogr.OFTReal))
    target_result_point_layer.CreateField(
        ogr.FieldDefn(habitat_role, ogr.OFTReal))
    n_features = target_result_point_layer.GetFeatureCount()
    if n_features > 0:
        LOGGER.debug("calculating final risks")
        target_result_point_layer.StartTransaction()
        for target_feature in target_result_point_layer:
            r_list = numpy.array([
                target_feature.GetField(risk_id)
                for risk_id in risk_id_list])
            r_tot = numpy.prod(r_list)**(1./len(r_list))
            target_feature.SetField(final_risk, float(r_tot))
            # User's Guide: calculate "coastal_exposure_no_habitats"
            # by replacing Rhab with 5
            r_list_nohab = numpy.array([
                target_feature.GetField(risk_id)
                for risk_id in risk_id_list_no_hab] + [5.0])
            r_tot_no_hab = numpy.prod(r_list_nohab)**(1./len(r_list_nohab))
            target_feature.SetField(
                habitat_role, abs(float(r_tot) - float(r_tot_no_hab)))
            target_result_point_layer.SetFeature(target_feature)
        target_result_point_layer.CommitTransaction()
    target_result_point_layer = None
    target_point_vector.FlushCache()
    target_point_vector = None


def _rasterize_vector_onto_base(
        base_raster_path, base_vector_path, attribute_id,
        target_raster_path, filter_string=None):
    """Rasterize attribute from vector onto a copy of base.

    Parameters:
        base_raster_path (string): path to a base raster file
        attribute_id (string): id in `base_vector_path` to rasterize.
        target_raster_path (string): a copy of `base_raster_path` with
            `base_vector_path[attribute_id]` rasterized on top.
        filter_string (string): filtering string to select from farm layer

    Returns:
        None.
    """
    base_raster = gdal.OpenEx(base_raster_path, gdal.OF_RASTER)
    raster_driver = gdal.GetDriverByName('GTiff')
    target_raster = raster_driver.CreateCopy(target_raster_path, base_raster)
    base_raster = None

    vector = gdal.OpenEx(base_vector_path)
    layer = vector.GetLayer()

    if filter_string is not None:
        layer.SetAttributeFilter(str(filter_string))
    gdal.RasterizeLayer(
        target_raster, [1], layer,
        options=['ATTRIBUTE=%s' % attribute_id])
    target_raster.FlushCache()
    target_raster = None
    layer = None
    vector = None


def build_wwiii_rtree(wwiii_vector_path, wwiii_rtree_path):
    """Build RTree indexed by FID for points in `wwwiii_vector_path`."""
    base_wwiii_rtree_path = os.path.splitext(wwiii_rtree_path)[0]
    if os.path.exists(wwiii_rtree_path):
        for ext in ['.dat', '.idx']:
            os.remove(base_wwiii_rtree_path+ext)
    wwiii_rtree = rtree.index.Index(base_wwiii_rtree_path)

    wwiii_vector = gdal.OpenEx(wwiii_vector_path, gdal.OF_VECTOR)
    wwiii_layer = wwiii_vector.GetLayer()
    for wwiii_feature in wwiii_layer:
        wwiii_geometry = wwiii_feature.GetGeometryRef()
        wwiii_x = wwiii_geometry.GetX()
        wwiii_y = wwiii_geometry.GetY()
        wwiii_rtree.insert(
            wwiii_feature.GetFID(), (wwiii_x, wwiii_y, wwiii_x, wwiii_y))
    wwiii_layer = None
    wwiii_vector = None


def build_feature_bounding_box_rtree(vector_path, target_rtree_path):
    """Build an r-tree index of the global feature envelopes.

    Parameter:
        vector_path (string): path to vector to build bounding box index for
        target_rtree_path (string): path to ".dat" file to store the saved
            r-tree.  A ValueError is raised if this file already exists

    Returns:
        None.

    """
    # the input path has a .dat extension, but the `rtree` package only uses
    # the basename.  It's a quirk of the library, so we'll deal with it by
    # cutting off the extension.
    global_feature_index_base = os.path.splitext(
        target_rtree_path)[0]
    LOGGER.info("Building rtree index at %s", global_feature_index_base)
    if os.path.exists(target_rtree_path):
        for ext in ['.dat', '.idx']:
            os.remove(global_feature_index_base + ext)
    global_feature_index = rtree.index.Index(global_feature_index_base)

    global_vector = gdal.OpenEx(vector_path, gdal.OF_VECTOR)
    global_layer = global_vector.GetLayer()
    n_features = global_layer.GetFeatureCount()

    logger_callback = _make_logger_callback(
        'rTree construction %.2f%% complete', LOGGER)

    for feature_index, global_feature in enumerate(global_layer):
        feature_geometry = global_feature.GetGeometryRef()
        # format of envelope is [minx, maxx, miny, maxy]
        feature_envelope = feature_geometry.GetEnvelope()
        # format of tree bounding box is [minx, miny, maxx, maxy]
        global_feature_index.insert(
            global_feature.GetFID(), (
                feature_envelope[0], feature_envelope[2],
                feature_envelope[1], feature_envelope[3]))
        logger_callback(float(feature_index) / n_features)
    global_feature_index.close()


def make_shore_kernel(kernel_path):
    """Make a 3x3 raster with a 9 in the middle and 1s on the outside."""
    driver = gdal.GetDriverByName('GTiff')
    kernel_raster = driver.Create(
        kernel_path.encode('utf-8'), 3, 3, 1,
        gdal.GDT_Byte)

    # Make some kind of geotransform, it doesn't matter what but
    # will make GIS libraries behave better if it's all defined
    kernel_raster.SetGeoTransform([0, 1, 0, 0, 0, -1])
    srs = osr.SpatialReference()
    srs.SetWellKnownGeogCS('WGS84')
    kernel_raster.SetProjection(srs.ExportToWkt())

    kernel_band = kernel_raster.GetRasterBand(1)
    kernel_band.SetNoDataValue(127)
    kernel_band.WriteArray(numpy.array([[1, 1, 1], [1, 9, 1], [1, 1, 1]]))


def create_averaging_kernel_raster(radius_in_pixels, kernel_filepath):
    """Create a raster kernel with a radius given.

    Parameters:
        expected_distance (int or float): The distance (in pixels) of the
            kernel's radius, the distance at which the value of the decay
            function is equal to `1/e`.
        kernel_filepath (string): The path to the file on disk where this
            kernel should be stored.  If this file exists, it will be
            overwritten.

    Returns:
        None

    """
    driver = gdal.GetDriverByName('GTiff')
    kernel_dataset = driver.Create(
        kernel_filepath.encode('utf-8'), int(radius_in_pixels)*2+1,
        int(radius_in_pixels)*2+1,
        1, gdal.GDT_Float32, options=[
            'BIGTIFF=IF_SAFER', 'TILED=YES', 'BLOCKXSIZE=256',
            'BLOCKYSIZE=256'])

    # Make some kind of geotransform, it doesn't matter what but
    # will make GIS libraries behave better if it's all defined
    kernel_dataset.SetGeoTransform([444720, 30, 0, 3751320, 0, -30])
    srs = osr.SpatialReference()
    srs.SetUTM(11, 1)
    srs.SetWellKnownGeogCS('NAD27')
    kernel_dataset.SetProjection(srs.ExportToWkt())

    kernel_band = kernel_dataset.GetRasterBand(1)
    kernel_band.SetNoDataValue(-9999)

    cols_per_block, rows_per_block = kernel_band.GetBlockSize()

    n_cols = kernel_dataset.RasterXSize
    n_rows = kernel_dataset.RasterYSize

    n_col_blocks = int(math.ceil(n_cols / float(cols_per_block)))
    n_row_blocks = int(math.ceil(n_rows / float(rows_per_block)))

    integration = 0.0
    for row_block_index in range(n_row_blocks):
        row_offset = row_block_index * rows_per_block
        row_block_width = n_rows - row_offset
        if row_block_width > rows_per_block:
            row_block_width = rows_per_block

        for col_block_index in range(n_col_blocks):
            col_offset = col_block_index * cols_per_block
            col_block_width = n_cols - col_offset
            if col_block_width > cols_per_block:
                col_block_width = cols_per_block

            # Numpy creates index rasters as ints by default, which sometimes
            # creates problems on 32-bit builds when we try to add Int32
            # matrices to float64 matrices.
            row_indices, col_indices = numpy.indices((row_block_width,
                                                      col_block_width),
                                                     dtype=numpy.float)

            row_indices += numpy.float(row_offset - radius_in_pixels)
            col_indices += numpy.float(col_offset - radius_in_pixels)

            kernel_index_distances = numpy.hypot(
                row_indices, col_indices)
            kernel = numpy.where(
                kernel_index_distances > radius_in_pixels, 0.0, 1.0)
            integration += numpy.sum(kernel)

            kernel_band.WriteArray(kernel, xoff=col_offset,
                                   yoff=row_offset)

    # Need to flush the kernel's cache to disk before opening up a new Dataset
    # object in interblocks()
    kernel_dataset.FlushCache()

    for block_data, kernel_block in pygeoprocessing.iterblocks(
            (kernel_filepath, 1)):
        kernel_block /= integration
        kernel_band.WriteArray(kernel_block, xoff=block_data['xoff'],
                               yoff=block_data['yoff'])


def geometry_to_lines(geometry):
    """Convert a geometry object to a list of lines."""
    if geometry.type == 'Polygon':
        return polygon_to_lines(geometry)
    elif geometry.type == 'MultiPolygon':
        line_list = []
        for geom in geometry.geoms:
            line_list.extend(geometry_to_lines(geom))
        return line_list
    else:
        return []


def polygon_to_lines(geometry):
    """Return a list of shapely lines given higher order shapely geometry."""
    line_list = []
    last_point = geometry.exterior.coords[0]
    for point in geometry.exterior.coords[1::]:
        if point == last_point:
            continue
        line_list.append(shapely.geometry.LineString([last_point, point]))
        last_point = point
    line_list.append(shapely.geometry.LineString([
        last_point, geometry.exterior.coords[0]]))
    for interior in geometry.interiors:
        last_point = interior.coords[0]
        for point in interior.coords[1::]:
            if point == last_point:
                continue
            line_list.append(shapely.geometry.LineString([last_point, point]))
            last_point = point
        line_list.append(shapely.geometry.LineString([
            last_point, interior.coords[0]]))
    return line_list


def _sanitize_path(base_path, raw_path):
    """Return `path` if absolute, or make absolute local to `base_path`."""
    if os.path.isabs(raw_path):
        return raw_path
    else:  # assume relative path w.r.t. the response table
        return os.path.join(os.path.dirname(base_path), raw_path)


def _make_logger_callback(message, logger):
    """Build a timed logger callback that prints `message` replaced.

    Parameters:
        message (string): a string that expects a %f replacement variable for
            `proportion_complete`.

    Returns:
        Function with signature:
            logger_callback(proportion_complete, psz_message, p_progress_arg)

    """
    def logger_callback(proportion_complete):
        """Argument names come from the GDAL API for callbacks."""
        try:
            current_time = time.time()
            if ((current_time - logger_callback.last_time) > 5.0 or
                    (proportion_complete == 1.0 and
                     logger_callback.total_time >= 5.0)):
                LOGGER.info(message, proportion_complete * 100)
                logger_callback.last_time = current_time
                logger_callback.total_time += current_time
        except AttributeError:
            logger_callback.last_time = time.time()
            logger_callback.total_time = 0.0

    return logger_callback